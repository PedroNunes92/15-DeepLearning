{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from numpy import vstack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Praticando Keras e PyTorch\n",
    "* Estrutura de Gradiente e Otimizador no PyTorch\n",
    "* CallBack - tensorboard para Keras e Pytorch\n",
    "* Classificação e Regressão no Keras\n",
    "* Classificação e Regressão no PyTorch\n",
    "* Inspeção de gradiente e Parâmetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atualização de gradiente embutido no tensor\n",
    "* requires_grad inclui um parametro de gradiente no tensor. \n",
    "* Caso um tensor realize um cálculo com um tensor que possa o parametro requires_grad = true, ao chamar o método backward desse tensor, o parametro grad dos tensores associados serão atualizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação com Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('pima-indians-diabetes.csv') \n",
    "print(df.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column = ['Class'] \n",
    "predictors = list(set(list(df.columns))-set(target_column))\n",
    "df[predictors] = df[predictors]/df[predictors].max()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[predictors].values\n",
    "y = df[target_column].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "count_classes = y_test.shape[1]\n",
    "print(count_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callback para tensorboard\n",
    "* tensorboard_callback\n",
    "* criar objeto callback\n",
    "* referenciar na chamada do fit  callbacks=[tensorboard_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logDir = \"/home/silvio/tb/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logDir)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=200, callbacks=[tensorboard_callback], validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão com Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.515869</td>\n",
       "      <td>5.458438</td>\n",
       "      <td>193.532746</td>\n",
       "      <td>104.209068</td>\n",
       "      <td>2970.261965</td>\n",
       "      <td>15.555668</td>\n",
       "      <td>75.994962</td>\n",
       "      <td>1.574307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.825804</td>\n",
       "      <td>1.701577</td>\n",
       "      <td>104.379583</td>\n",
       "      <td>38.338262</td>\n",
       "      <td>847.904119</td>\n",
       "      <td>2.749995</td>\n",
       "      <td>3.690005</td>\n",
       "      <td>0.802549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>1613.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>2223.000000</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>2800.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>262.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>3609.000000</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>46.600000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>5140.000000</td>\n",
       "      <td>24.800000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mpg   cylinders  displacement  horsepower       weight  \\\n",
       "count  397.000000  397.000000    397.000000  397.000000   397.000000   \n",
       "mean    23.515869    5.458438    193.532746  104.209068  2970.261965   \n",
       "std      7.825804    1.701577    104.379583   38.338262   847.904119   \n",
       "min      9.000000    3.000000     68.000000   46.000000  1613.000000   \n",
       "25%     17.500000    4.000000    104.000000   75.000000  2223.000000   \n",
       "50%     23.000000    4.000000    146.000000   93.000000  2800.000000   \n",
       "75%     29.000000    8.000000    262.000000  125.000000  3609.000000   \n",
       "max     46.600000    8.000000    455.000000  230.000000  5140.000000   \n",
       "\n",
       "       acceleration        year      origin  \n",
       "count    397.000000  397.000000  397.000000  \n",
       "mean      15.555668   75.994962    1.574307  \n",
       "std        2.749995    3.690005    0.802549  \n",
       "min        8.000000   70.000000    1.000000  \n",
       "25%       13.800000   73.000000    1.000000  \n",
       "50%       15.500000   76.000000    1.000000  \n",
       "75%       17.100000   79.000000    2.000000  \n",
       "max       24.800000   82.000000    3.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Auto2.csv') \n",
    "print(df.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>originL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86</td>\n",
       "      <td>2790</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52</td>\n",
       "      <td>2130</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84</td>\n",
       "      <td>2295</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79</td>\n",
       "      <td>2625</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82</td>\n",
       "      <td>2720</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0    18.0          8         307.0         130    3504          12.0    70   \n",
       "1    15.0          8         350.0         165    3693          11.5    70   \n",
       "2    18.0          8         318.0         150    3436          11.0    70   \n",
       "3    16.0          8         304.0         150    3433          12.0    70   \n",
       "4    17.0          8         302.0         140    3449          10.5    70   \n",
       "..    ...        ...           ...         ...     ...           ...   ...   \n",
       "392  27.0          4         140.0          86    2790          15.6    82   \n",
       "393  44.0          4          97.0          52    2130          24.6    82   \n",
       "394  32.0          4         135.0          84    2295          11.6    82   \n",
       "395  28.0          4         120.0          79    2625          18.6    82   \n",
       "396  31.0          4         119.0          82    2720          19.4    82   \n",
       "\n",
       "     originL  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "..       ...  \n",
       "392        0  \n",
       "393        1  \n",
       "394        0  \n",
       "395        0  \n",
       "396        0  \n",
       "\n",
       "[397 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['originL']= le.fit_transform(df['origin'])\n",
    "\n",
    "df=df.drop(columns = ['name','origin'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>originL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.515869</td>\n",
       "      <td>0.682305</td>\n",
       "      <td>0.425347</td>\n",
       "      <td>0.453083</td>\n",
       "      <td>0.577872</td>\n",
       "      <td>0.627245</td>\n",
       "      <td>0.926768</td>\n",
       "      <td>0.287154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.825804</td>\n",
       "      <td>0.212697</td>\n",
       "      <td>0.229406</td>\n",
       "      <td>0.166688</td>\n",
       "      <td>0.164962</td>\n",
       "      <td>0.110887</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.401275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.149451</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.313813</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.432490</td>\n",
       "      <td>0.556452</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.320879</td>\n",
       "      <td>0.404348</td>\n",
       "      <td>0.544747</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.575824</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.702140</td>\n",
       "      <td>0.689516</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>46.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mpg   cylinders  displacement  horsepower      weight  \\\n",
       "count  397.000000  397.000000    397.000000  397.000000  397.000000   \n",
       "mean    23.515869    0.682305      0.425347    0.453083    0.577872   \n",
       "std      7.825804    0.212697      0.229406    0.166688    0.164962   \n",
       "min      9.000000    0.375000      0.149451    0.200000    0.313813   \n",
       "25%     17.500000    0.500000      0.228571    0.326087    0.432490   \n",
       "50%     23.000000    0.500000      0.320879    0.404348    0.544747   \n",
       "75%     29.000000    1.000000      0.575824    0.543478    0.702140   \n",
       "max     46.600000    1.000000      1.000000    1.000000    1.000000   \n",
       "\n",
       "       acceleration        year     originL  \n",
       "count    397.000000  397.000000  397.000000  \n",
       "mean       0.627245    0.926768    0.287154  \n",
       "std        0.110887    0.045000    0.401275  \n",
       "min        0.322581    0.853659    0.000000  \n",
       "25%        0.556452    0.890244    0.000000  \n",
       "50%        0.625000    0.926829    0.000000  \n",
       "75%        0.689516    0.963415    0.500000  \n",
       "max        1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = ['mpg'] \n",
    "predictors = list(set(list(df.columns))-set(target_column))\n",
    "df[predictors] = df[predictors]/df[predictors].max()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(277, 7)\n",
      "(120, 7)\n"
     ]
    }
   ],
   "source": [
    "X = df[predictors].values\n",
    "y = df[target_column].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "7/7 [==============================] - 0s 46ms/step - loss: 555.1578 - mae: 22.2754 - mse: 555.1578 - val_loss: 644.5776 - val_mae: 24.0928 - val_mse: 644.5776\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 523.3012 - mae: 21.5267 - mse: 523.3012 - val_loss: 615.1154 - val_mae: 23.4521 - val_mse: 615.1154\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 493.4888 - mae: 20.8164 - mse: 493.4888 - val_loss: 575.3471 - val_mae: 22.5750 - val_mse: 575.3471\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 456.7764 - mae: 19.8757 - mse: 456.7764 - val_loss: 537.0754 - val_mae: 21.6834 - val_mse: 537.0754\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 419.5083 - mae: 18.8956 - mse: 419.5083 - val_loss: 488.2349 - val_mae: 20.5041 - val_mse: 488.2349\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 374.7316 - mae: 17.6557 - mse: 374.7316 - val_loss: 436.3003 - val_mae: 19.1604 - val_mse: 436.3003\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 327.8645 - mae: 16.2139 - mse: 327.8645 - val_loss: 381.9821 - val_mae: 17.6478 - val_mse: 381.9821\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 280.0051 - mae: 14.6359 - mse: 280.0051 - val_loss: 322.7944 - val_mae: 15.8334 - val_mse: 322.7944\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 231.1147 - mae: 12.8090 - mse: 231.1147 - val_loss: 276.9988 - val_mae: 14.3049 - val_mse: 276.9988\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 192.9110 - mae: 11.2982 - mse: 192.9110 - val_loss: 229.4669 - val_mae: 12.7055 - val_mse: 229.4669\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 156.2947 - mae: 9.8358 - mse: 156.2947 - val_loss: 185.3715 - val_mae: 11.2962 - val_mse: 185.3715\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 125.3307 - mae: 8.8067 - mse: 125.3307 - val_loss: 149.3323 - val_mae: 10.0660 - val_mse: 149.3323\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 102.3452 - mae: 8.0367 - mse: 102.3452 - val_loss: 120.3475 - val_mae: 9.0271 - val_mse: 120.3475\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 88.2993 - mae: 7.6230 - mse: 88.2993 - val_loss: 108.0005 - val_mae: 8.5687 - val_mse: 108.0005\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 82.2778 - mae: 7.4252 - mse: 82.2778 - val_loss: 102.7728 - val_mae: 8.3461 - val_mse: 102.7728\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 78.7309 - mae: 7.3154 - mse: 78.7309 - val_loss: 96.6293 - val_mae: 8.1099 - val_mse: 96.6293\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 75.4733 - mae: 7.1150 - mse: 75.4733 - val_loss: 86.5335 - val_mae: 7.7796 - val_mse: 86.5335\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 71.3596 - mae: 6.9567 - mse: 71.3596 - val_loss: 88.3407 - val_mae: 7.7380 - val_mse: 88.3407\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 69.0094 - mae: 6.7935 - mse: 69.0094 - val_loss: 77.3490 - val_mae: 7.4038 - val_mse: 77.3490\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 65.9491 - mae: 6.6783 - mse: 65.9491 - val_loss: 75.6317 - val_mae: 7.2938 - val_mse: 75.6317\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 63.3469 - mae: 6.5462 - mse: 63.3469 - val_loss: 69.3917 - val_mae: 7.0485 - val_mse: 69.3917\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 60.7835 - mae: 6.4438 - mse: 60.7835 - val_loss: 72.9214 - val_mae: 7.0521 - val_mse: 72.9214\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 58.4058 - mae: 6.2371 - mse: 58.4058 - val_loss: 65.0182 - val_mae: 6.8159 - val_mse: 65.0182\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 56.6755 - mae: 6.2031 - mse: 56.6755 - val_loss: 66.4765 - val_mae: 6.7685 - val_mse: 66.4765\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 54.2824 - mae: 6.0332 - mse: 54.2824 - val_loss: 62.6328 - val_mae: 6.6164 - val_mse: 62.6328\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 52.3908 - mae: 5.9290 - mse: 52.3908 - val_loss: 58.2607 - val_mae: 6.4828 - val_mse: 58.2607\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 51.5640 - mae: 5.9420 - mse: 51.5640 - val_loss: 59.6476 - val_mae: 6.3918 - val_mse: 59.6476\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 48.4201 - mae: 5.6823 - mse: 48.4201 - val_loss: 61.1588 - val_mae: 6.3510 - val_mse: 61.1588\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 47.3774 - mae: 5.5509 - mse: 47.3774 - val_loss: 54.1933 - val_mae: 6.1883 - val_mse: 54.1933\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 45.8217 - mae: 5.5509 - mse: 45.8217 - val_loss: 53.3358 - val_mae: 6.1109 - val_mse: 53.3358\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 44.2226 - mae: 5.4265 - mse: 44.2226 - val_loss: 52.6834 - val_mae: 6.0142 - val_mse: 52.6834\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 42.6472 - mae: 5.3150 - mse: 42.6472 - val_loss: 55.3985 - val_mae: 5.9640 - val_mse: 55.3985\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 41.5542 - mae: 5.1397 - mse: 41.5542 - val_loss: 52.3154 - val_mae: 5.8215 - val_mse: 52.3154\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 39.8690 - mae: 5.0114 - mse: 39.8690 - val_loss: 47.4863 - val_mae: 5.7820 - val_mse: 47.4863\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 39.0007 - mae: 5.0535 - mse: 39.0007 - val_loss: 48.8839 - val_mae: 5.6500 - val_mse: 48.8839\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 37.4347 - mae: 4.8468 - mse: 37.4347 - val_loss: 46.0010 - val_mae: 5.6048 - val_mse: 46.0010\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 36.4422 - mae: 4.8362 - mse: 36.4422 - val_loss: 45.1553 - val_mae: 5.5029 - val_mse: 45.1553\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 35.3251 - mae: 4.7207 - mse: 35.3251 - val_loss: 46.2346 - val_mae: 5.4183 - val_mse: 46.2346\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 34.6079 - mae: 4.5735 - mse: 34.6079 - val_loss: 46.6706 - val_mae: 5.3718 - val_mse: 46.6706\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 33.7384 - mae: 4.4783 - mse: 33.7384 - val_loss: 42.5868 - val_mae: 5.3225 - val_mse: 42.5868\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 32.7248 - mae: 4.4802 - mse: 32.7248 - val_loss: 44.3048 - val_mae: 5.2142 - val_mse: 44.3048\n",
      "Epoch 42/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 31.9944 - mae: 4.3205 - mse: 31.9944 - val_loss: 43.3908 - val_mae: 5.1444 - val_mse: 43.3908\n",
      "Epoch 43/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 31.1338 - mae: 4.2322 - mse: 31.1338 - val_loss: 40.8453 - val_mae: 5.1672 - val_mse: 40.8453\n",
      "Epoch 44/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 30.5646 - mae: 4.2448 - mse: 30.5646 - val_loss: 41.6761 - val_mae: 5.0240 - val_mse: 41.6761\n",
      "Epoch 45/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 29.4971 - mae: 4.0973 - mse: 29.4971 - val_loss: 42.5026 - val_mae: 4.9854 - val_mse: 42.5026\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 29.4782 - mae: 4.0330 - mse: 29.4782 - val_loss: 40.4065 - val_mae: 4.9435 - val_mse: 40.4065\n",
      "Epoch 47/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 28.6027 - mae: 3.9961 - mse: 28.6027 - val_loss: 39.1978 - val_mae: 5.0990 - val_mse: 39.1978\n",
      "Epoch 48/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 28.3290 - mae: 4.1252 - mse: 28.3290 - val_loss: 38.3943 - val_mae: 4.9372 - val_mse: 38.3943\n",
      "Epoch 49/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 27.4996 - mae: 3.9803 - mse: 27.4996 - val_loss: 41.2230 - val_mae: 4.8509 - val_mse: 41.2230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 28.1885 - mae: 3.8799 - mse: 28.1885 - val_loss: 39.0034 - val_mae: 4.7965 - val_mse: 39.0034\n",
      "Epoch 51/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 27.0865 - mae: 3.8311 - mse: 27.0865 - val_loss: 37.5567 - val_mae: 4.7970 - val_mse: 37.5567\n",
      "Epoch 52/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 26.6577 - mae: 3.8497 - mse: 26.6577 - val_loss: 37.0516 - val_mae: 4.7992 - val_mse: 37.0516\n",
      "Epoch 53/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 26.4363 - mae: 3.8463 - mse: 26.4363 - val_loss: 36.8721 - val_mae: 4.8785 - val_mse: 36.8721\n",
      "Epoch 54/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 26.1350 - mae: 3.8447 - mse: 26.1350 - val_loss: 36.8961 - val_mae: 4.6678 - val_mse: 36.8961\n",
      "Epoch 55/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 25.4369 - mae: 3.6941 - mse: 25.4369 - val_loss: 37.8528 - val_mae: 4.6611 - val_mse: 37.8528\n",
      "Epoch 56/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 25.9568 - mae: 3.6940 - mse: 25.9568 - val_loss: 35.7672 - val_mae: 4.6385 - val_mse: 35.7672\n",
      "Epoch 57/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 25.1156 - mae: 3.7078 - mse: 25.1156 - val_loss: 35.7818 - val_mae: 4.6149 - val_mse: 35.7818\n",
      "Epoch 58/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 24.8193 - mae: 3.6747 - mse: 24.8193 - val_loss: 36.1751 - val_mae: 4.5990 - val_mse: 36.1751\n",
      "Epoch 59/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 24.7960 - mae: 3.6512 - mse: 24.7960 - val_loss: 36.3346 - val_mae: 4.5863 - val_mse: 36.3346\n",
      "Epoch 60/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 24.8493 - mae: 3.6369 - mse: 24.8493 - val_loss: 38.4545 - val_mae: 4.6580 - val_mse: 38.4545\n",
      "Epoch 61/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 25.5356 - mae: 3.6856 - mse: 25.5356 - val_loss: 36.7440 - val_mae: 4.5717 - val_mse: 36.7440\n",
      "Epoch 62/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 24.5310 - mae: 3.6128 - mse: 24.5310 - val_loss: 36.3856 - val_mae: 4.5562 - val_mse: 36.3856\n",
      "Epoch 63/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 24.8075 - mae: 3.6366 - mse: 24.8075 - val_loss: 35.2282 - val_mae: 4.5114 - val_mse: 35.2282\n",
      "Epoch 64/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 23.8932 - mae: 3.5782 - mse: 23.8932 - val_loss: 36.3484 - val_mae: 4.5379 - val_mse: 36.3484\n",
      "Epoch 65/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 24.3085 - mae: 3.5831 - mse: 24.3085 - val_loss: 33.7443 - val_mae: 4.4552 - val_mse: 33.7443\n",
      "Epoch 66/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 23.3683 - mae: 3.5785 - mse: 23.3683 - val_loss: 32.9981 - val_mae: 4.4564 - val_mse: 32.9981\n",
      "Epoch 67/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 22.9189 - mae: 3.5303 - mse: 22.9189 - val_loss: 34.9567 - val_mae: 4.4613 - val_mse: 34.9567\n",
      "Epoch 68/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 23.4599 - mae: 3.5668 - mse: 23.4599 - val_loss: 32.5251 - val_mae: 4.5405 - val_mse: 32.5251\n",
      "Epoch 69/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 22.7020 - mae: 3.5525 - mse: 22.7020 - val_loss: 32.1785 - val_mae: 4.4158 - val_mse: 32.1785\n",
      "Epoch 70/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 22.3884 - mae: 3.4974 - mse: 22.3884 - val_loss: 32.5119 - val_mae: 4.3696 - val_mse: 32.5119\n",
      "Epoch 71/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 22.3194 - mae: 3.4716 - mse: 22.3194 - val_loss: 31.8386 - val_mae: 4.3605 - val_mse: 31.8386\n",
      "Epoch 72/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 22.0602 - mae: 3.4528 - mse: 22.0602 - val_loss: 33.1515 - val_mae: 4.3755 - val_mse: 33.1515\n",
      "Epoch 73/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 22.2751 - mae: 3.4494 - mse: 22.2751 - val_loss: 31.1361 - val_mae: 4.3368 - val_mse: 31.1361\n",
      "Epoch 74/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 21.6647 - mae: 3.4166 - mse: 21.6647 - val_loss: 30.7490 - val_mae: 4.3586 - val_mse: 30.7490\n",
      "Epoch 75/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 21.6607 - mae: 3.4279 - mse: 21.6607 - val_loss: 31.5959 - val_mae: 4.2853 - val_mse: 31.5959\n",
      "Epoch 76/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 21.9166 - mae: 3.4322 - mse: 21.9166 - val_loss: 30.8482 - val_mae: 4.2513 - val_mse: 30.8482\n",
      "Epoch 77/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 21.2763 - mae: 3.3704 - mse: 21.2763 - val_loss: 29.9971 - val_mae: 4.3499 - val_mse: 29.9971\n",
      "Epoch 78/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 20.9763 - mae: 3.3985 - mse: 20.9763 - val_loss: 30.2556 - val_mae: 4.2050 - val_mse: 30.2556\n",
      "Epoch 79/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 21.1482 - mae: 3.3604 - mse: 21.1482 - val_loss: 30.5004 - val_mae: 4.2111 - val_mse: 30.5004\n",
      "Epoch 80/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 20.9564 - mae: 3.3686 - mse: 20.9564 - val_loss: 28.8694 - val_mae: 4.2368 - val_mse: 28.8694\n",
      "Epoch 81/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 20.4816 - mae: 3.3671 - mse: 20.4816 - val_loss: 29.1586 - val_mae: 4.1376 - val_mse: 29.1586\n",
      "Epoch 82/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 20.1091 - mae: 3.3132 - mse: 20.1091 - val_loss: 28.7528 - val_mae: 4.2928 - val_mse: 28.7528\n",
      "Epoch 83/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 20.5498 - mae: 3.3911 - mse: 20.5498 - val_loss: 30.0629 - val_mae: 4.1736 - val_mse: 30.0629\n",
      "Epoch 84/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 20.3433 - mae: 3.3051 - mse: 20.3433 - val_loss: 28.1989 - val_mae: 4.0876 - val_mse: 28.1989\n",
      "Epoch 85/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 19.7466 - mae: 3.2784 - mse: 19.7466 - val_loss: 28.6011 - val_mae: 4.0884 - val_mse: 28.6011\n",
      "Epoch 86/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 19.7166 - mae: 3.2637 - mse: 19.7166 - val_loss: 28.8395 - val_mae: 4.0973 - val_mse: 28.8395\n",
      "Epoch 87/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 19.8286 - mae: 3.2628 - mse: 19.8286 - val_loss: 27.2798 - val_mae: 4.0967 - val_mse: 27.2798\n",
      "Epoch 88/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 19.5389 - mae: 3.2506 - mse: 19.5389 - val_loss: 27.5760 - val_mae: 4.0231 - val_mse: 27.5760\n",
      "Epoch 89/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 19.2772 - mae: 3.2276 - mse: 19.2772 - val_loss: 27.4889 - val_mae: 4.0099 - val_mse: 27.4889\n",
      "Epoch 90/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 19.3032 - mae: 3.2314 - mse: 19.3032 - val_loss: 26.7640 - val_mae: 3.9961 - val_mse: 26.7640\n",
      "Epoch 91/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 19.0860 - mae: 3.2153 - mse: 19.0860 - val_loss: 26.5108 - val_mae: 3.9896 - val_mse: 26.5108\n",
      "Epoch 92/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 18.9717 - mae: 3.2076 - mse: 18.9717 - val_loss: 27.3051 - val_mae: 3.9897 - val_mse: 27.3051\n",
      "Epoch 93/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 19.0769 - mae: 3.2156 - mse: 19.0769 - val_loss: 26.7087 - val_mae: 4.1407 - val_mse: 26.7087\n",
      "Epoch 94/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 19.4111 - mae: 3.3181 - mse: 19.4111 - val_loss: 26.1268 - val_mae: 3.9239 - val_mse: 26.1268\n",
      "Epoch 95/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 18.6303 - mae: 3.1855 - mse: 18.6303 - val_loss: 28.1010 - val_mae: 3.9920 - val_mse: 28.1010\n",
      "Epoch 96/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 19.4289 - mae: 3.2175 - mse: 19.4289 - val_loss: 25.4292 - val_mae: 4.0036 - val_mse: 25.4292\n",
      "Epoch 97/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 18.6017 - mae: 3.2224 - mse: 18.6017 - val_loss: 25.4558 - val_mae: 4.0114 - val_mse: 25.4558\n",
      "Epoch 98/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 18.7053 - mae: 3.2181 - mse: 18.7053 - val_loss: 25.4368 - val_mae: 4.0158 - val_mse: 25.4368\n",
      "Epoch 99/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 13ms/step - loss: 18.6571 - mae: 3.2248 - mse: 18.6571 - val_loss: 25.2447 - val_mae: 3.8904 - val_mse: 25.2447\n",
      "Epoch 100/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 18.2893 - mae: 3.1655 - mse: 18.2893 - val_loss: 25.7337 - val_mae: 4.0445 - val_mse: 25.7337\n",
      "Epoch 101/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 18.5305 - mae: 3.2165 - mse: 18.5305 - val_loss: 25.4917 - val_mae: 3.8954 - val_mse: 25.4917\n",
      "Epoch 102/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 18.4212 - mae: 3.1786 - mse: 18.4212 - val_loss: 24.9555 - val_mae: 3.9548 - val_mse: 24.9555\n",
      "Epoch 103/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 18.2736 - mae: 3.1686 - mse: 18.2736 - val_loss: 25.4028 - val_mae: 3.8700 - val_mse: 25.4028\n",
      "Epoch 104/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 18.1470 - mae: 3.1457 - mse: 18.1470 - val_loss: 24.7157 - val_mae: 3.9316 - val_mse: 24.7157\n",
      "Epoch 105/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 18.0234 - mae: 3.1592 - mse: 18.0234 - val_loss: 24.7898 - val_mae: 3.8475 - val_mse: 24.7898\n",
      "Epoch 106/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 18.2685 - mae: 3.1705 - mse: 18.2685 - val_loss: 27.2978 - val_mae: 3.9665 - val_mse: 27.2978\n",
      "Epoch 107/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 18.8794 - mae: 3.2227 - mse: 18.8794 - val_loss: 24.7083 - val_mae: 3.8360 - val_mse: 24.7083\n",
      "Epoch 108/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 17.9997 - mae: 3.1380 - mse: 17.9997 - val_loss: 24.4008 - val_mae: 3.9102 - val_mse: 24.4008\n",
      "Epoch 109/200\n",
      "7/7 [==============================] - 0s 40ms/step - loss: 17.8173 - mae: 3.1483 - mse: 17.8173 - val_loss: 25.7743 - val_mae: 3.8819 - val_mse: 25.7743\n",
      "Epoch 110/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 18.2413 - mae: 3.1525 - mse: 18.2413 - val_loss: 24.3078 - val_mae: 3.8082 - val_mse: 24.3078\n",
      "Epoch 111/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 17.7284 - mae: 3.1389 - mse: 17.7284 - val_loss: 25.4916 - val_mae: 4.0362 - val_mse: 25.4916\n",
      "Epoch 112/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 18.2646 - mae: 3.2220 - mse: 18.2646 - val_loss: 24.7395 - val_mae: 3.9584 - val_mse: 24.7395\n",
      "Epoch 113/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 18.0804 - mae: 3.1831 - mse: 18.0804 - val_loss: 24.2446 - val_mae: 3.7986 - val_mse: 24.2446\n",
      "Epoch 114/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 17.6122 - mae: 3.1183 - mse: 17.6122 - val_loss: 23.7102 - val_mae: 3.7812 - val_mse: 23.7102\n",
      "Epoch 115/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 17.4178 - mae: 3.1127 - mse: 17.4178 - val_loss: 23.9469 - val_mae: 3.8718 - val_mse: 23.9469\n",
      "Epoch 116/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 17.9148 - mae: 3.1576 - mse: 17.9148 - val_loss: 23.4651 - val_mae: 3.7827 - val_mse: 23.4651\n",
      "Epoch 117/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 17.4900 - mae: 3.1034 - mse: 17.4900 - val_loss: 24.7105 - val_mae: 3.9606 - val_mse: 24.7105\n",
      "Epoch 118/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 17.9731 - mae: 3.1749 - mse: 17.9731 - val_loss: 25.5237 - val_mae: 4.0342 - val_mse: 25.5237\n",
      "Epoch 119/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 18.2139 - mae: 3.2266 - mse: 18.2139 - val_loss: 23.8928 - val_mae: 3.8697 - val_mse: 23.8928\n",
      "Epoch 120/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 17.6263 - mae: 3.1113 - mse: 17.6263 - val_loss: 23.3693 - val_mae: 3.8107 - val_mse: 23.3693\n",
      "Epoch 121/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 17.4977 - mae: 3.1210 - mse: 17.4977 - val_loss: 23.7026 - val_mae: 3.8583 - val_mse: 23.7026\n",
      "Epoch 122/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 17.3469 - mae: 3.1085 - mse: 17.3469 - val_loss: 22.9651 - val_mae: 3.7202 - val_mse: 22.9651\n",
      "Epoch 123/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 17.1789 - mae: 3.0830 - mse: 17.1789 - val_loss: 24.1511 - val_mae: 3.9151 - val_mse: 24.1511\n",
      "Epoch 124/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 17.8382 - mae: 3.1502 - mse: 17.8382 - val_loss: 22.8135 - val_mae: 3.7459 - val_mse: 22.8135\n",
      "Epoch 125/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 17.2425 - mae: 3.0933 - mse: 17.2425 - val_loss: 22.6384 - val_mae: 3.7022 - val_mse: 22.6384\n",
      "Epoch 126/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 16.9499 - mae: 3.0360 - mse: 16.9499 - val_loss: 23.0461 - val_mae: 3.7983 - val_mse: 23.0461\n",
      "Epoch 127/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 17.3841 - mae: 3.1330 - mse: 17.3841 - val_loss: 22.2930 - val_mae: 3.6655 - val_mse: 22.2930\n",
      "Epoch 128/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 16.8450 - mae: 3.0682 - mse: 16.8450 - val_loss: 22.6140 - val_mae: 3.6413 - val_mse: 22.6140\n",
      "Epoch 129/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 16.7957 - mae: 3.0526 - mse: 16.7957 - val_loss: 23.7971 - val_mae: 3.6805 - val_mse: 23.7971\n",
      "Epoch 130/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 17.1551 - mae: 3.0944 - mse: 17.1551 - val_loss: 22.4400 - val_mae: 3.6058 - val_mse: 22.4400\n",
      "Epoch 131/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 16.7042 - mae: 3.0288 - mse: 16.7042 - val_loss: 22.8437 - val_mae: 3.6352 - val_mse: 22.8437\n",
      "Epoch 132/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 16.6939 - mae: 3.0408 - mse: 16.6939 - val_loss: 22.1447 - val_mae: 3.7192 - val_mse: 22.1447\n",
      "Epoch 133/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 16.8400 - mae: 3.0903 - mse: 16.8400 - val_loss: 23.1056 - val_mae: 3.6356 - val_mse: 23.1056\n",
      "Epoch 134/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 16.4810 - mae: 2.9989 - mse: 16.4810 - val_loss: 21.6583 - val_mae: 3.6442 - val_mse: 21.6583\n",
      "Epoch 135/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 16.3385 - mae: 3.0265 - mse: 16.3385 - val_loss: 21.3040 - val_mae: 3.5829 - val_mse: 21.3040\n",
      "Epoch 136/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 16.4068 - mae: 3.0309 - mse: 16.4068 - val_loss: 22.6666 - val_mae: 3.5799 - val_mse: 22.6666\n",
      "Epoch 137/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 16.5534 - mae: 3.0297 - mse: 16.5534 - val_loss: 21.0155 - val_mae: 3.5185 - val_mse: 21.0155\n",
      "Epoch 138/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 16.1187 - mae: 2.9957 - mse: 16.1187 - val_loss: 23.3304 - val_mae: 3.6265 - val_mse: 23.3304\n",
      "Epoch 139/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 16.6122 - mae: 3.0249 - mse: 16.6122 - val_loss: 23.3231 - val_mae: 3.6307 - val_mse: 23.3231\n",
      "Epoch 140/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 16.6286 - mae: 3.0379 - mse: 16.6286 - val_loss: 21.2567 - val_mae: 3.5158 - val_mse: 21.2567\n",
      "Epoch 141/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 16.0419 - mae: 2.9682 - mse: 16.0419 - val_loss: 22.8434 - val_mae: 3.8234 - val_mse: 22.8434\n",
      "Epoch 142/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 16.9227 - mae: 3.1152 - mse: 16.9227 - val_loss: 20.7578 - val_mae: 3.4849 - val_mse: 20.7578\n",
      "Epoch 143/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 15.8476 - mae: 2.9644 - mse: 15.8476 - val_loss: 21.2732 - val_mae: 3.4891 - val_mse: 21.2732\n",
      "Epoch 144/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 15.9768 - mae: 2.9758 - mse: 15.9768 - val_loss: 20.7773 - val_mae: 3.5704 - val_mse: 20.7773\n",
      "Epoch 145/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 16.0750 - mae: 3.0153 - mse: 16.0750 - val_loss: 20.7441 - val_mae: 3.4500 - val_mse: 20.7441\n",
      "Epoch 146/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 15.7249 - mae: 2.9417 - mse: 15.7249 - val_loss: 20.2801 - val_mae: 3.4646 - val_mse: 20.2801\n",
      "Epoch 147/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 16.1194 - mae: 2.9728 - mse: 16.1194 - val_loss: 21.3543 - val_mae: 3.4868 - val_mse: 21.3543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 16.0008 - mae: 2.9864 - mse: 16.0008 - val_loss: 20.8134 - val_mae: 3.5772 - val_mse: 20.8134\n",
      "Epoch 149/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 16.0686 - mae: 3.0102 - mse: 16.0686 - val_loss: 20.4256 - val_mae: 3.5122 - val_mse: 20.4256\n",
      "Epoch 150/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 15.7510 - mae: 2.9726 - mse: 15.7510 - val_loss: 20.6356 - val_mae: 3.5586 - val_mse: 20.6356\n",
      "Epoch 151/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 15.9966 - mae: 3.0027 - mse: 15.9966 - val_loss: 20.7894 - val_mae: 3.4542 - val_mse: 20.7894\n",
      "Epoch 152/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 15.8544 - mae: 2.9716 - mse: 15.8544 - val_loss: 20.9366 - val_mae: 3.6090 - val_mse: 20.9366\n",
      "Epoch 153/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 15.9911 - mae: 2.9944 - mse: 15.9911 - val_loss: 20.1561 - val_mae: 3.4788 - val_mse: 20.1561\n",
      "Epoch 154/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 15.6301 - mae: 2.9639 - mse: 15.6301 - val_loss: 20.0744 - val_mae: 3.4302 - val_mse: 20.0744\n",
      "Epoch 155/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 15.6654 - mae: 2.9398 - mse: 15.6654 - val_loss: 21.4847 - val_mae: 3.4912 - val_mse: 21.4847\n",
      "Epoch 156/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 15.7375 - mae: 2.9428 - mse: 15.7375 - val_loss: 20.7028 - val_mae: 3.4393 - val_mse: 20.7028\n",
      "Epoch 157/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 15.4546 - mae: 2.9320 - mse: 15.4546 - val_loss: 19.9344 - val_mae: 3.4070 - val_mse: 19.9344\n",
      "Epoch 158/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 15.4059 - mae: 2.9249 - mse: 15.4059 - val_loss: 21.4126 - val_mae: 3.4764 - val_mse: 21.4126\n",
      "Epoch 159/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 15.6518 - mae: 2.9652 - mse: 15.6518 - val_loss: 20.2992 - val_mae: 3.3974 - val_mse: 20.2992\n",
      "Epoch 160/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 15.4004 - mae: 2.9104 - mse: 15.4004 - val_loss: 19.7031 - val_mae: 3.3755 - val_mse: 19.7031\n",
      "Epoch 161/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 15.3228 - mae: 2.9350 - mse: 15.3228 - val_loss: 19.8059 - val_mae: 3.4779 - val_mse: 19.8059\n",
      "Epoch 162/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 15.2441 - mae: 2.9060 - mse: 15.2441 - val_loss: 19.6446 - val_mae: 3.3695 - val_mse: 19.6446\n",
      "Epoch 163/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 15.0198 - mae: 2.9077 - mse: 15.0198 - val_loss: 19.6840 - val_mae: 3.4533 - val_mse: 19.6840\n",
      "Epoch 164/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 15.1834 - mae: 2.9407 - mse: 15.1834 - val_loss: 19.7185 - val_mae: 3.3883 - val_mse: 19.7185\n",
      "Epoch 165/200\n",
      "7/7 [==============================] - 0s 16ms/step - loss: 15.1737 - mae: 2.9069 - mse: 15.1737 - val_loss: 19.5075 - val_mae: 3.4444 - val_mse: 19.5075\n",
      "Epoch 166/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 15.1318 - mae: 2.9152 - mse: 15.1318 - val_loss: 22.4608 - val_mae: 3.7774 - val_mse: 22.4608\n",
      "Epoch 167/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 16.1830 - mae: 3.0573 - mse: 16.1830 - val_loss: 19.7306 - val_mae: 3.3968 - val_mse: 19.7306\n",
      "Epoch 168/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 14.9041 - mae: 2.8784 - mse: 14.9041 - val_loss: 19.7396 - val_mae: 3.4060 - val_mse: 19.7396\n",
      "Epoch 169/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 14.8554 - mae: 2.8834 - mse: 14.8554 - val_loss: 19.2017 - val_mae: 3.3795 - val_mse: 19.2017\n",
      "Epoch 170/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 14.7939 - mae: 2.8737 - mse: 14.7939 - val_loss: 19.4072 - val_mae: 3.4356 - val_mse: 19.4072\n",
      "Epoch 171/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 14.8929 - mae: 2.8538 - mse: 14.8929 - val_loss: 18.9673 - val_mae: 3.3510 - val_mse: 18.9673\n",
      "Epoch 172/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 14.7193 - mae: 2.8596 - mse: 14.7193 - val_loss: 20.5312 - val_mae: 3.4350 - val_mse: 20.5312\n",
      "Epoch 173/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 14.8582 - mae: 2.8839 - mse: 14.8582 - val_loss: 20.2979 - val_mae: 3.4059 - val_mse: 20.2979\n",
      "Epoch 174/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 14.8023 - mae: 2.8602 - mse: 14.8023 - val_loss: 20.0457 - val_mae: 3.5213 - val_mse: 20.0457\n",
      "Epoch 175/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 15.1791 - mae: 2.9490 - mse: 15.1791 - val_loss: 19.4949 - val_mae: 3.4617 - val_mse: 19.4949\n",
      "Epoch 176/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 14.7997 - mae: 2.8979 - mse: 14.7997 - val_loss: 19.7148 - val_mae: 3.3876 - val_mse: 19.7148\n",
      "Epoch 177/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 14.7952 - mae: 2.8801 - mse: 14.7952 - val_loss: 19.9238 - val_mae: 3.4963 - val_mse: 19.9238\n",
      "Epoch 178/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 15.0625 - mae: 2.9240 - mse: 15.0625 - val_loss: 18.9759 - val_mae: 3.3629 - val_mse: 18.9759\n",
      "Epoch 179/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 14.4346 - mae: 2.8504 - mse: 14.4346 - val_loss: 20.3762 - val_mae: 3.4304 - val_mse: 20.3762\n",
      "Epoch 180/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 14.8930 - mae: 2.8624 - mse: 14.8930 - val_loss: 19.1634 - val_mae: 3.3406 - val_mse: 19.1634\n",
      "Epoch 181/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 14.4540 - mae: 2.8431 - mse: 14.4540 - val_loss: 19.0256 - val_mae: 3.3397 - val_mse: 19.0256\n",
      "Epoch 182/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 14.3916 - mae: 2.8249 - mse: 14.3916 - val_loss: 18.4646 - val_mae: 3.3121 - val_mse: 18.4646\n",
      "Epoch 183/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 14.4052 - mae: 2.8396 - mse: 14.4052 - val_loss: 18.7038 - val_mae: 3.2902 - val_mse: 18.7038\n",
      "Epoch 184/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 14.3251 - mae: 2.8304 - mse: 14.3251 - val_loss: 18.7135 - val_mae: 3.3769 - val_mse: 18.7135\n",
      "Epoch 185/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 14.7990 - mae: 2.9084 - mse: 14.7990 - val_loss: 18.3387 - val_mae: 3.2748 - val_mse: 18.3387\n",
      "Epoch 186/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 14.2213 - mae: 2.8119 - mse: 14.2213 - val_loss: 18.3216 - val_mae: 3.3136 - val_mse: 18.3216\n",
      "Epoch 187/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 14.3146 - mae: 2.8434 - mse: 14.3146 - val_loss: 19.1150 - val_mae: 3.4319 - val_mse: 19.1150\n",
      "Epoch 188/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 14.7401 - mae: 2.8552 - mse: 14.7401 - val_loss: 18.1462 - val_mae: 3.2753 - val_mse: 18.1462\n",
      "Epoch 189/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 14.0337 - mae: 2.8137 - mse: 14.0337 - val_loss: 18.1096 - val_mae: 3.2809 - val_mse: 18.1096\n",
      "Epoch 190/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 13.9303 - mae: 2.8092 - mse: 13.9303 - val_loss: 20.0707 - val_mae: 3.3880 - val_mse: 20.0707\n",
      "Epoch 191/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 14.4176 - mae: 2.7981 - mse: 14.4176 - val_loss: 19.2747 - val_mae: 3.4333 - val_mse: 19.2747\n",
      "Epoch 192/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 14.1286 - mae: 2.8265 - mse: 14.1286 - val_loss: 18.1111 - val_mae: 3.2610 - val_mse: 18.1111\n",
      "Epoch 193/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 14.0879 - mae: 2.8050 - mse: 14.0879 - val_loss: 18.4489 - val_mae: 3.2855 - val_mse: 18.4489\n",
      "Epoch 194/200\n",
      "7/7 [==============================] - 0s 15ms/step - loss: 13.9083 - mae: 2.7948 - mse: 13.9083 - val_loss: 18.9726 - val_mae: 3.3300 - val_mse: 18.9726\n",
      "Epoch 195/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 14.1945 - mae: 2.8233 - mse: 14.1945 - val_loss: 17.9869 - val_mae: 3.2553 - val_mse: 17.9869\n",
      "Epoch 196/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 13.8161 - mae: 2.7911 - mse: 13.8161 - val_loss: 20.6621 - val_mae: 3.4480 - val_mse: 20.6621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 197/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 14.5585 - mae: 2.8250 - mse: 14.5585 - val_loss: 18.7835 - val_mae: 3.2598 - val_mse: 18.7835\n",
      "Epoch 198/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 13.8993 - mae: 2.7725 - mse: 13.8993 - val_loss: 17.7997 - val_mae: 3.1981 - val_mse: 17.7997\n",
      "Epoch 199/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 13.9724 - mae: 2.8041 - mse: 13.9724 - val_loss: 18.2789 - val_mae: 3.2298 - val_mse: 18.2789\n",
      "Epoch 200/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 13.7048 - mae: 2.7890 - mse: 13.7048 - val_loss: 17.7496 - val_mae: 3.2666 - val_mse: 17.7496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffa281a3e50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logDir = \"/home/silvio/tb/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logDir)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=200, callbacks=[tensorboard_callback], validation_split=0.3)\n",
    "#model.fit(X_train, y_train, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação com Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callback para tensorboard - Pytorch\n",
    "* criar objeto de escrita no tensorboard\n",
    "    * writerTest = SummaryWriter(logDir)\n",
    "* adicionar entrada no tensorboard (ex: uma vez por época)\n",
    "    * writerTest.add_scalar('epoch_accuracy', acctest, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        self.X = df.values[:, :-1]\n",
    "        self.y = df.values[:, -1]\n",
    "        \n",
    "        self.X = self.X.astype('float32')\n",
    "\n",
    "        sc = StandardScaler()\n",
    "        self.X = sc.fit_transform(self.X)\n",
    "\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    " \n",
    "    # quantas linhas tem no dataset?\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # obtem uma linha do dataset\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    # retorna base para treino e teste\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        return random_split(self, [train_size, test_size])\n",
    "    \n",
    "def prepare_data(path):\n",
    "    # Carrega Dataset\n",
    "    dataset = DiabetesDataset(path)\n",
    "    # realiza split\n",
    "    train, test = dataset.get_splits()\n",
    "    # monta data loaders\n",
    "    train_dl = DataLoader(train, batch_size=1024, shuffle=True)\n",
    "    #test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    \n",
    "    return train_dl, test_dl\n",
    "\n",
    "class MLP(Module):\n",
    "    # Elementos do modelo\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # camada de entrada\n",
    "        self.hidden1 = Linear(n_inputs, 64)\n",
    "        # Inicialização da camada de entrada\n",
    "        #kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        # Ativação da camada de entrada\n",
    "        self.act1 = ReLU()\n",
    "        # segunda camada , entrada tem que ser do mesmo tamanho da saida da camada 1\n",
    "        self.hidden2 = Linear(64, 8)\n",
    "        # Inicialização da camada\n",
    "        #kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        # Ativação da camada\n",
    "        self.act2 = ReLU()\n",
    "        self.hidden3 = Linear(8, 8)\n",
    "        self.act3 = ReLU()\n",
    "        \n",
    "        # camada de saída\n",
    "        self.hidden4 = Linear(8, 1)\n",
    "        # Inicialização da camada\n",
    "        #xavier_uniform_(self.hidden3.weight)\n",
    "        # Ativação da camada\n",
    "        self.act4 = Sigmoid()\n",
    " \n",
    "    # propagação da entrada pelas camadas\n",
    "    def forward(self, X):\n",
    "        # entrada para primeira camada escondida\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        # segunda camada escondida\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # terceira camada escondida\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        # camada de saida\n",
    "        X = self.hidden4(X)\n",
    "        X = self.act4(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "logDir = \"/home/silvio/tb/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/train/\"\n",
    "\n",
    "writerTrain = SummaryWriter(logDir)\n",
    "logDir = \"/home/silvio/tb/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/test/\"\n",
    "writerTest = SummaryWriter(logDir)\n",
    "\n",
    "def train_model(train_dl, model):\n",
    "    # define loss\n",
    "    criterion = BCELoss()\n",
    "    # define otimizador\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # loop por épocas\n",
    "    for epoch in range(200):\n",
    "        # Loop em conjunto de mini-batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # zera os gradientes do batches\n",
    "            optimizer.zero_grad()\n",
    "            # predição do batch\n",
    "            yhat = model(inputs)\n",
    "            # calcula loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            \n",
    "            \n",
    "            #print(yhat.detach().numpy(), targets.detach().numpy())\n",
    "            #print(accuracy_score(yhat.detach().numpy(), targets.detach().numpy()))\n",
    "            \n",
    "            #print(type(yhat), type(targets))\n",
    "            #print()\n",
    "            \n",
    "            #writer.add_scalar('Loss ', loss.detach().numpy(),epoch)    \n",
    "            \n",
    "            # Retroprapagando erros \n",
    "            loss.backward()\n",
    "            # atualiza pesos (otimização )\n",
    "            optimizer.step()\n",
    "            #print(loss)\n",
    "            \n",
    "            acctest = evaluate_model(test_dl, model)\n",
    "            acctrain = evaluate_model(train_dl, model)\n",
    "            #writer.add_scalar('epoch_loss', loss.detach().numpy(), epoch)\n",
    "            writerTrain.add_scalar('epoch_accuracy', acctrain, epoch)\n",
    "            #writer.add_scalar('epoch_accuracy', acctest, epoch)\n",
    "            writerTrain.add_scalar('epoch_loss', loss.detach().numpy(), epoch)\n",
    "            writerTest.add_scalar('epoch_accuracy', acctest, epoch)\n",
    "            \n",
    "            print(\"epoca \" , epoch, \" loss \", loss.detach().numpy() , \" acctest \", acctest,\" acctrain \", acctrain)\n",
    "\n",
    "\n",
    "def evaluate_model(test_dl, model):\n",
    "    # Cria lista de preditos e reais\n",
    "    predictions, actuals = list(), list()\n",
    "\n",
    "    # percorre lista do dataloader de test\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # realiza predição\n",
    "        yhat = model(inputs)\n",
    "        # cria numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round valores da classe\n",
    "        yhat = yhat.round()\n",
    "        # armazena\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "        \n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # usa sklearn para calcular acurácia\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515 253\n",
      "epoca  0  loss  0.6901595  acctest  0.541501976284585  acctrain  0.6038834951456311\n",
      "epoca  1  loss  0.6896743  acctest  0.5889328063241107  acctrain  0.6194174757281553\n",
      "epoca  2  loss  0.6887585  acctest  0.6324110671936759  acctrain  0.6349514563106796\n",
      "epoca  3  loss  0.68746644  acctest  0.6600790513833992  acctrain  0.6349514563106796\n",
      "epoca  4  loss  0.6858494  acctest  0.6561264822134387  acctrain  0.6388349514563106\n",
      "epoca  5  loss  0.68395394  acctest  0.6798418972332015  acctrain  0.6679611650485436\n",
      "epoca  6  loss  0.6818271  acctest  0.6758893280632411  acctrain  0.6776699029126214\n",
      "epoca  7  loss  0.6795147  acctest  0.6719367588932806  acctrain  0.6718446601941748\n",
      "epoca  8  loss  0.6770553  acctest  0.6482213438735178  acctrain  0.6679611650485436\n",
      "epoca  9  loss  0.6744856  acctest  0.6482213438735178  acctrain  0.6601941747572816\n",
      "epoca  10  loss  0.6718457  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  11  loss  0.6691664  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  12  loss  0.6664817  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  13  loss  0.66381  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  14  loss  0.6611682  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  15  loss  0.65857065  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  16  loss  0.6560336  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  17  loss  0.6535699  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  18  loss  0.6511841  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  19  loss  0.64888424  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  20  loss  0.6466761  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  21  loss  0.64455885  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  22  loss  0.6425365  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  23  loss  0.6406071  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  24  loss  0.6387699  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  25  loss  0.63701946  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  26  loss  0.63535714  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  27  loss  0.6337825  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  28  loss  0.6322918  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  29  loss  0.630882  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  30  loss  0.6295468  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  31  loss  0.62827814  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  32  loss  0.6270735  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  33  loss  0.6259211  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  34  loss  0.6248096  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  35  loss  0.6237392  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  36  loss  0.6227066  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  37  loss  0.62170684  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  38  loss  0.6207372  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  39  loss  0.6198001  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  40  loss  0.6188819  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  41  loss  0.61797684  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  42  loss  0.61708224  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  43  loss  0.6161923  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  44  loss  0.61530375  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  45  loss  0.6144206  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  46  loss  0.6135344  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  47  loss  0.6126477  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  48  loss  0.6117537  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  49  loss  0.6108452  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  50  loss  0.60994035  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  51  loss  0.60903084  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  52  loss  0.60812205  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  53  loss  0.60720193  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  54  loss  0.6062746  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  55  loss  0.6053268  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  56  loss  0.60435855  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  57  loss  0.6033714  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  58  loss  0.6023714  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  59  loss  0.60136175  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  60  loss  0.60033464  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  61  loss  0.5992857  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  62  loss  0.59821725  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  63  loss  0.5971239  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  64  loss  0.5960235  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  65  loss  0.59491116  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  66  loss  0.5937658  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  67  loss  0.59258485  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  68  loss  0.59137076  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  69  loss  0.5901315  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  70  loss  0.58886963  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  71  loss  0.5875901  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  72  loss  0.58629143  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  73  loss  0.58496034  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  74  loss  0.58360267  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  75  loss  0.582208  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  76  loss  0.58079505  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  77  loss  0.5793529  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  78  loss  0.5778922  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  79  loss  0.57641137  acctest  0.6363636363636364  acctrain  0.6601941747572816\n",
      "epoca  80  loss  0.5749138  acctest  0.6363636363636364  acctrain  0.6601941747572816\n",
      "epoca  81  loss  0.5733898  acctest  0.6363636363636364  acctrain  0.6601941747572816\n",
      "epoca  82  loss  0.5718409  acctest  0.6363636363636364  acctrain  0.6601941747572816\n",
      "epoca  83  loss  0.5702636  acctest  0.6363636363636364  acctrain  0.6621359223300971\n",
      "epoca  84  loss  0.5686596  acctest  0.6363636363636364  acctrain  0.6621359223300971\n",
      "epoca  85  loss  0.5670344  acctest  0.6363636363636364  acctrain  0.6621359223300971\n",
      "epoca  86  loss  0.5653991  acctest  0.6363636363636364  acctrain  0.6621359223300971\n",
      "epoca  87  loss  0.56375414  acctest  0.6403162055335968  acctrain  0.6621359223300971\n",
      "epoca  88  loss  0.56211  acctest  0.6442687747035574  acctrain  0.6621359223300971\n",
      "epoca  89  loss  0.56046164  acctest  0.6442687747035574  acctrain  0.6679611650485436\n",
      "epoca  90  loss  0.5588091  acctest  0.6482213438735178  acctrain  0.6699029126213593\n",
      "epoca  91  loss  0.5571459  acctest  0.6521739130434783  acctrain  0.6679611650485436\n",
      "epoca  92  loss  0.5554656  acctest  0.6521739130434783  acctrain  0.6718446601941748\n",
      "epoca  93  loss  0.5537656  acctest  0.6561264822134387  acctrain  0.6718446601941748\n",
      "epoca  94  loss  0.55206704  acctest  0.6561264822134387  acctrain  0.6757281553398058\n",
      "epoca  95  loss  0.550359  acctest  0.6600790513833992  acctrain  0.6757281553398058\n",
      "epoca  96  loss  0.5486689  acctest  0.6640316205533597  acctrain  0.6776699029126214\n",
      "epoca  97  loss  0.5469894  acctest  0.6679841897233202  acctrain  0.6796116504854369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoca  98  loss  0.54533064  acctest  0.6679841897233202  acctrain  0.6815533980582524\n",
      "epoca  99  loss  0.5436632  acctest  0.6679841897233202  acctrain  0.6873786407766991\n",
      "epoca  100  loss  0.54202205  acctest  0.6758893280632411  acctrain  0.6912621359223301\n",
      "epoca  101  loss  0.5403822  acctest  0.6758893280632411  acctrain  0.6990291262135923\n",
      "epoca  102  loss  0.5387523  acctest  0.6798418972332015  acctrain  0.7048543689320388\n",
      "epoca  103  loss  0.53712547  acctest  0.6837944664031621  acctrain  0.7067961165048544\n",
      "epoca  104  loss  0.53552854  acctest  0.6996047430830039  acctrain  0.7048543689320388\n",
      "epoca  105  loss  0.53396595  acctest  0.6996047430830039  acctrain  0.7067961165048544\n",
      "epoca  106  loss  0.53241795  acctest  0.6996047430830039  acctrain  0.7126213592233009\n",
      "epoca  107  loss  0.53087735  acctest  0.6956521739130435  acctrain  0.7106796116504854\n",
      "epoca  108  loss  0.5293534  acctest  0.6996047430830039  acctrain  0.7145631067961165\n",
      "epoca  109  loss  0.5278419  acctest  0.7035573122529645  acctrain  0.7203883495145631\n",
      "epoca  110  loss  0.5263458  acctest  0.7035573122529645  acctrain  0.7262135922330097\n",
      "epoca  111  loss  0.52486676  acctest  0.7075098814229249  acctrain  0.7281553398058253\n",
      "epoca  112  loss  0.5234101  acctest  0.7114624505928854  acctrain  0.7281553398058253\n",
      "epoca  113  loss  0.5219619  acctest  0.7154150197628458  acctrain  0.7320388349514563\n",
      "epoca  114  loss  0.52053064  acctest  0.7193675889328063  acctrain  0.7339805825242719\n",
      "epoca  115  loss  0.5191173  acctest  0.7233201581027668  acctrain  0.7398058252427184\n",
      "epoca  116  loss  0.51771116  acctest  0.7233201581027668  acctrain  0.7417475728155339\n",
      "epoca  117  loss  0.516326  acctest  0.7193675889328063  acctrain  0.7456310679611651\n",
      "epoca  118  loss  0.5149726  acctest  0.7233201581027668  acctrain  0.7456310679611651\n",
      "epoca  119  loss  0.5136451  acctest  0.7193675889328063  acctrain  0.7475728155339806\n",
      "epoca  120  loss  0.51234245  acctest  0.7233201581027668  acctrain  0.7495145631067961\n",
      "epoca  121  loss  0.51105475  acctest  0.7312252964426877  acctrain  0.7495145631067961\n",
      "epoca  122  loss  0.509798  acctest  0.7312252964426877  acctrain  0.7553398058252427\n",
      "epoca  123  loss  0.5085686  acctest  0.7312252964426877  acctrain  0.7572815533980582\n",
      "epoca  124  loss  0.5073651  acctest  0.7272727272727273  acctrain  0.7572815533980582\n",
      "epoca  125  loss  0.5061854  acctest  0.7312252964426877  acctrain  0.7572815533980582\n",
      "epoca  126  loss  0.5050334  acctest  0.7312252964426877  acctrain  0.7611650485436893\n",
      "epoca  127  loss  0.50390685  acctest  0.7272727272727273  acctrain  0.7611650485436893\n",
      "epoca  128  loss  0.50279546  acctest  0.7272727272727273  acctrain  0.7631067961165049\n",
      "epoca  129  loss  0.5017015  acctest  0.7312252964426877  acctrain  0.7650485436893204\n",
      "epoca  130  loss  0.5006275  acctest  0.7391304347826086  acctrain  0.7689320388349514\n",
      "epoca  131  loss  0.4995775  acctest  0.7430830039525692  acctrain  0.7689320388349514\n",
      "epoca  132  loss  0.4985582  acctest  0.7470355731225297  acctrain  0.7689320388349514\n",
      "epoca  133  loss  0.49756405  acctest  0.7509881422924901  acctrain  0.7669902912621359\n",
      "epoca  134  loss  0.49658957  acctest  0.7549407114624506  acctrain  0.7669902912621359\n",
      "epoca  135  loss  0.49562573  acctest  0.7509881422924901  acctrain  0.7650485436893204\n",
      "epoca  136  loss  0.49467695  acctest  0.7549407114624506  acctrain  0.7631067961165049\n",
      "epoca  137  loss  0.49374372  acctest  0.7549407114624506  acctrain  0.7631067961165049\n",
      "epoca  138  loss  0.49282953  acctest  0.7549407114624506  acctrain  0.7631067961165049\n",
      "epoca  139  loss  0.49193707  acctest  0.7549407114624506  acctrain  0.7650485436893204\n",
      "epoca  140  loss  0.49106365  acctest  0.758893280632411  acctrain  0.7650485436893204\n",
      "epoca  141  loss  0.49020386  acctest  0.7628458498023716  acctrain  0.7631067961165049\n",
      "epoca  142  loss  0.4893568  acctest  0.766798418972332  acctrain  0.7669902912621359\n",
      "epoca  143  loss  0.48853016  acctest  0.766798418972332  acctrain  0.7669902912621359\n",
      "epoca  144  loss  0.48772556  acctest  0.7628458498023716  acctrain  0.7669902912621359\n",
      "epoca  145  loss  0.4869431  acctest  0.766798418972332  acctrain  0.7708737864077669\n",
      "epoca  146  loss  0.48618183  acctest  0.766798418972332  acctrain  0.7708737864077669\n",
      "epoca  147  loss  0.48543817  acctest  0.766798418972332  acctrain  0.7708737864077669\n",
      "epoca  148  loss  0.48471376  acctest  0.766798418972332  acctrain  0.7747572815533981\n",
      "epoca  149  loss  0.4840115  acctest  0.766798418972332  acctrain  0.7747572815533981\n",
      "epoca  150  loss  0.4833265  acctest  0.766798418972332  acctrain  0.7747572815533981\n",
      "epoca  151  loss  0.48265997  acctest  0.766798418972332  acctrain  0.7708737864077669\n",
      "epoca  152  loss  0.4820071  acctest  0.766798418972332  acctrain  0.7689320388349514\n",
      "epoca  153  loss  0.48137182  acctest  0.7707509881422925  acctrain  0.7708737864077669\n",
      "epoca  154  loss  0.4807552  acctest  0.7707509881422925  acctrain  0.7708737864077669\n",
      "epoca  155  loss  0.48015013  acctest  0.7707509881422925  acctrain  0.7708737864077669\n",
      "epoca  156  loss  0.47955945  acctest  0.7707509881422925  acctrain  0.7708737864077669\n",
      "epoca  157  loss  0.47898346  acctest  0.7707509881422925  acctrain  0.7689320388349514\n",
      "epoca  158  loss  0.47842282  acctest  0.7707509881422925  acctrain  0.7689320388349514\n",
      "epoca  159  loss  0.4778752  acctest  0.7747035573122529  acctrain  0.7728155339805826\n",
      "epoca  160  loss  0.47734398  acctest  0.7747035573122529  acctrain  0.7728155339805826\n",
      "epoca  161  loss  0.47682786  acctest  0.7747035573122529  acctrain  0.7689320388349514\n",
      "epoca  162  loss  0.47632366  acctest  0.7747035573122529  acctrain  0.7689320388349514\n",
      "epoca  163  loss  0.47583294  acctest  0.7747035573122529  acctrain  0.7689320388349514\n",
      "epoca  164  loss  0.47535887  acctest  0.7747035573122529  acctrain  0.7689320388349514\n",
      "epoca  165  loss  0.47489804  acctest  0.7747035573122529  acctrain  0.7669902912621359\n",
      "epoca  166  loss  0.4744472  acctest  0.7786561264822134  acctrain  0.7669902912621359\n",
      "epoca  167  loss  0.47400543  acctest  0.782608695652174  acctrain  0.7689320388349514\n",
      "epoca  168  loss  0.4735689  acctest  0.782608695652174  acctrain  0.7708737864077669\n",
      "epoca  169  loss  0.47314405  acctest  0.782608695652174  acctrain  0.7728155339805826\n",
      "epoca  170  loss  0.47272488  acctest  0.782608695652174  acctrain  0.7728155339805826\n",
      "epoca  171  loss  0.4723093  acctest  0.7786561264822134  acctrain  0.7728155339805826\n",
      "epoca  172  loss  0.47190106  acctest  0.7747035573122529  acctrain  0.7728155339805826\n",
      "epoca  173  loss  0.47150072  acctest  0.7747035573122529  acctrain  0.7728155339805826\n",
      "epoca  174  loss  0.47111169  acctest  0.7747035573122529  acctrain  0.7747572815533981\n",
      "epoca  175  loss  0.47072956  acctest  0.7747035573122529  acctrain  0.7766990291262136\n",
      "epoca  176  loss  0.470359  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "epoca  177  loss  0.46999514  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "epoca  178  loss  0.46963483  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "epoca  179  loss  0.46927962  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "epoca  180  loss  0.46893448  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "epoca  181  loss  0.4685964  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  182  loss  0.46826777  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  183  loss  0.4679518  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  184  loss  0.46764532  acctest  0.782608695652174  acctrain  0.7747572815533981\n",
      "epoca  185  loss  0.46734756  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "epoca  186  loss  0.46705666  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "epoca  187  loss  0.46676153  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  188  loss  0.46647272  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  189  loss  0.4661888  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  190  loss  0.46590924  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  191  loss  0.4656336  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  192  loss  0.46536088  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  193  loss  0.4650917  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  194  loss  0.46482557  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  195  loss  0.46456522  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  196  loss  0.46431226  acctest  0.7786561264822134  acctrain  0.7766990291262136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoca  197  loss  0.46406308  acctest  0.7786561264822134  acctrain  0.7786407766990291\n",
      "epoca  198  loss  0.46381685  acctest  0.7786561264822134  acctrain  0.7786407766990291\n",
      "epoca  199  loss  0.46357346  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "Acuracia: 0.779\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Prepara os dados\n",
    "path = 'pima-indians-diabetes.csv'\n",
    "\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "\n",
    "# criar modelo\n",
    "model = MLP(8)\n",
    "# treina o modelo\n",
    "train_model(train_dl, model)\n",
    "# avalia\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Acuracia: %.3f' % acc)\n",
    "# testa uma predição\n",
    "#row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "#yhat = predict(row, model)\n",
    "#print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão com PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        df['originL']= le.fit_transform(df['origin'])\n",
    "\n",
    "        df=df.drop(columns = ['name','origin'])\n",
    "        #df\n",
    "\n",
    "        target_column = ['mpg'] \n",
    "        predictors = list(set(list(df.columns))-set(target_column))\n",
    "        df[predictors] = df[predictors]/df[predictors].max()\n",
    "        #df.describe()\n",
    "\n",
    "        X = df[predictors].values\n",
    "        y = df[target_column].values\n",
    "\n",
    "\n",
    "        X = X.astype('float32')\n",
    "        y = y.astype('float32')\n",
    "\n",
    "\n",
    "        sc = StandardScaler()\n",
    "        X = sc.fit_transform(X)\n",
    "\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "        #print(X_train.shape); print(X_test.shape)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        '''\n",
    "        self.X = df.values[:, :-1]\n",
    "        self.y = df.values[:, -1]\n",
    "        \n",
    "        self.X = self.X.astype('float32')\n",
    "\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "        '''\n",
    " \n",
    "    # quantas linhas tem no dataset?\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # obtem uma linha do dataset\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    # retorna base para treino e teste\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        return random_split(self, [train_size, test_size])\n",
    "    \n",
    "def prepare_data(path):\n",
    "    # Carrega Dataset\n",
    "    dataset = AutoDataset(path)\n",
    "    # realiza split\n",
    "    train, test = dataset.get_splits()\n",
    "    # monta data loaders\n",
    "    train_dl = DataLoader(train, batch_size=1024, shuffle=True)\n",
    "    #train_dl = DataLoader(train)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    #test_dl = DataLoader(test)\n",
    "    #test_dl = DataLoader(test, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train_dl, test_dl\n",
    "\n",
    "class MLP(Module):\n",
    "    # Elementos do modelo\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # camada de entrada\n",
    "        self.hidden1 = Linear(n_inputs, 360)\n",
    "        # Inicialização da camada de entrada\n",
    "        #kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        # Ativação da camada de entrada\n",
    "        self.act1 = ReLU()\n",
    "        # segunda camada , entrada tem que ser do mesmo tamanho da saida da camada 1\n",
    "        self.hidden2 = Linear(360, 128)\n",
    "        # Inicialização da camada\n",
    "        #kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        # Ativação da camada\n",
    "        self.act2 = ReLU()\n",
    "        # camada de saída\n",
    "        self.hidden3 = Linear(128, 1)\n",
    "        # Inicialização da camada\n",
    "        #xavier_uniform_(self.hidden3.weight)\n",
    "        # Ativação da camada\n",
    "        self.act3 = ReLU()\n",
    " \n",
    "    # propagação da entrada pelas camadas\n",
    "    def forward(self, X):\n",
    "        #print(X.shape)\n",
    "        # entrada para primeira camada escondida\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        # segunda camada escondida\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # terceira camada escondida\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X\n",
    "\n",
    "logDir = \"/home/silvio/tb/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/train/\"\n",
    "\n",
    "writerTrain = SummaryWriter(logDir)\n",
    "logDir = \"/home/silvio/tb/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/test/\"\n",
    "writerTest = SummaryWriter(logDir)\n",
    "\n",
    "def train_model(train_dl, model):\n",
    "    # define loss\n",
    "    criterion = MSELoss()\n",
    "    # define otimizador\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # loop por épocas\n",
    "    for epoch in range(200):\n",
    "        \n",
    "        # Loop em conjunto de mini-batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            yhat = model(inputs)\n",
    "            # calcula loss\n",
    "            loss = criterion(yhat, targets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #writer.add_scalar('epoch/mse', loss.detach().numpy(), epoch)\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "            #print('gradiente do peso antes backward', model.hidden1.weight.grad)\n",
    "            #print('pesos antes backward', model.hidden1.weight)\n",
    "            #print('gradiente do bias antes backward', model.hidden1.bias.grad)\n",
    "            #print('bias antes backward', model.hidden1.bias)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            #print('gradiente do peso depois backward', model.hidden1.weight.grad)\n",
    "            #print('pesos depois backward', model.hidden1.weight)\n",
    "            #print('gradiente do bias depois backward', model.hidden1.bias.grad)\n",
    "            #print('bias depois backward', model.hidden1.bias)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            #print('pesos depois passo de otimização', model.hidden1.weight)\n",
    "            #print('bias depois passo de otimização', model.hidden1.bias)\n",
    "            mse=evaluate_model(test_dl, model)\n",
    "            print(\"epoca \" , epoch, \" loss \", loss.detach().numpy(), \"mse test\", mse)\n",
    "            \n",
    "            writerTrain.add_scalar('epoch_mse', loss.detach().numpy(), epoch)\n",
    "            writerTest.add_scalar('epoch_mse', mse, epoch)\n",
    "            \n",
    "def evaluate_model(test_dl, model):\n",
    "    # Cria lista de preditos e reais\n",
    "    predictions, actuals = list(), list()\n",
    "\n",
    "    # percorre lista do dataloader de test\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # realiza predição\n",
    "        yhat = model(inputs)\n",
    "        # cria numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round valores da classe\n",
    "        yhat = yhat.round()\n",
    "        # armazena\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "        \n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # usa sklearn para calcular acurácia\n",
    "    acc = mean_squared_error(actuals, predictions)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266 131\n",
      "epoca  0  loss  608.40704 mse test 441.3485\n",
      "epoca  1  loss  439.77414 mse test 892.1211\n",
      "epoca  2  loss  962.41907 mse test 527.21576\n",
      "epoca  3  loss  545.64075 mse test 522.59436\n",
      "epoca  4  loss  519.58026 mse test 480.5226\n",
      "epoca  5  loss  475.7317 mse test 391.5287\n",
      "epoca  6  loss  397.59586 mse test 243.51495\n",
      "epoca  7  loss  236.2251 mse test 383.69055\n",
      "epoca  8  loss  418.2335 mse test 517.03253\n",
      "epoca  9  loss  525.11566 mse test 523.9272\n",
      "epoca  10  loss  531.06354 mse test 266.2188\n",
      "epoca  11  loss  267.804 mse test 191.87679\n",
      "epoca  12  loss  186.42989 mse test 149.73328\n",
      "epoca  13  loss  151.41153 mse test 102.51955\n",
      "epoca  14  loss  105.10439 mse test 21.76687\n",
      "epoca  15  loss  25.298557 mse test 227.96384\n",
      "epoca  16  loss  260.91028 mse test 79.37451\n",
      "epoca  17  loss  86.02096 mse test 71.301216\n",
      "epoca  18  loss  73.01635 mse test 65.23099\n",
      "epoca  19  loss  67.6661 mse test 61.159237\n",
      "epoca  20  loss  64.22483 mse test 59.087482\n",
      "epoca  21  loss  62.49419 mse test 59.01572\n",
      "epoca  22  loss  62.22071 mse test 58.49664\n",
      "epoca  23  loss  62.687546 mse test 53.50122\n",
      "epoca  24  loss  59.913754 mse test 104.56076\n",
      "epoca  25  loss  110.573 mse test 111.76228\n",
      "epoca  26  loss  111.77158 mse test 110.61267\n",
      "epoca  27  loss  112.73332 mse test 112.113434\n",
      "epoca  28  loss  112.84353 mse test 112.69512\n",
      "epoca  29  loss  111.63185 mse test 110.64626\n",
      "epoca  30  loss  110.13743 mse test 110.179085\n",
      "epoca  31  loss  108.33745 mse test 105.30122\n",
      "epoca  32  loss  105.886795 mse test 103.70733\n",
      "epoca  33  loss  104.11441 mse test 101.80046\n",
      "epoca  34  loss  101.32825 mse test 98.125656\n",
      "epoca  35  loss  99.17548 mse test 97.11497\n",
      "epoca  36  loss  97.14062 mse test 94.28596\n",
      "epoca  37  loss  95.17674 mse test 92.113434\n",
      "epoca  38  loss  93.53544 mse test 91.23711\n",
      "epoca  39  loss  92.48931 mse test 91.65847\n",
      "epoca  40  loss  91.884415 mse test 92.23253\n",
      "epoca  41  loss  91.509254 mse test 92.33786\n",
      "epoca  42  loss  91.04025 mse test 91.48901\n",
      "epoca  43  loss  89.85576 mse test 88.757706\n",
      "epoca  44  loss  88.05119 mse test 85.85694\n",
      "epoca  45  loss  87.21705 mse test 84.22641\n",
      "epoca  46  loss  87.38559 mse test 84.78367\n",
      "epoca  47  loss  87.625916 mse test 83.69817\n",
      "epoca  48  loss  86.67521 mse test 83.56992\n",
      "epoca  49  loss  85.29101 mse test 83.924126\n",
      "epoca  50  loss  84.94194 mse test 82.52564\n",
      "epoca  51  loss  84.32265 mse test 82.9913\n",
      "epoca  52  loss  83.671036 mse test 82.434044\n",
      "epoca  53  loss  83.19498 mse test 83.85237\n",
      "epoca  54  loss  82.13812 mse test 83.06917\n",
      "epoca  55  loss  82.395424 mse test 87.50428\n",
      "epoca  56  loss  84.6017 mse test 92.021835\n",
      "epoca  57  loss  96.562645 mse test 152.8371\n",
      "epoca  58  loss  147.81816 mse test 131.34856\n",
      "epoca  59  loss  141.80136 mse test 92.162285\n",
      "epoca  60  loss  97.10851 mse test 96.48901\n",
      "epoca  61  loss  99.89586 mse test 98.10122\n",
      "epoca  62  loss  101.67228 mse test 98.47221\n",
      "epoca  63  loss  101.36416 mse test 96.606575\n",
      "epoca  64  loss  100.52732 mse test 95.95466\n",
      "epoca  65  loss  99.16453 mse test 94.14244\n",
      "epoca  66  loss  97.27671 mse test 91.32412\n",
      "epoca  67  loss  95.01296 mse test 88.22947\n",
      "epoca  68  loss  92.58651 mse test 85.56075\n",
      "epoca  69  loss  90.15913 mse test 84.32259\n",
      "epoca  70  loss  89.35035 mse test 85.2203\n",
      "epoca  71  loss  90.347 mse test 86.36076\n",
      "epoca  72  loss  91.027245 mse test 85.40809\n",
      "epoca  73  loss  90.67754 mse test 84.8539\n",
      "epoca  74  loss  90.29095 mse test 84.35008\n",
      "epoca  75  loss  89.32677 mse test 82.76992\n",
      "epoca  76  loss  87.43736 mse test 81.618774\n",
      "epoca  77  loss  85.823364 mse test 81.92259\n",
      "epoca  78  loss  85.37957 mse test 81.89969\n",
      "epoca  79  loss  84.92746 mse test 81.08749\n",
      "epoca  80  loss  84.147606 mse test 80.6371\n",
      "epoca  81  loss  83.87643 mse test 80.9287\n",
      "epoca  82  loss  83.52084 mse test 80.42031\n",
      "epoca  83  loss  82.66422 mse test 80.67985\n",
      "epoca  84  loss  82.05536 mse test 82.29969\n",
      "epoca  85  loss  82.046135 mse test 81.62336\n",
      "epoca  86  loss  81.365654 mse test 81.763824\n",
      "epoca  87  loss  81.19852 mse test 81.991295\n",
      "epoca  88  loss  81.00072 mse test 82.4997\n",
      "epoca  89  loss  80.59586 mse test 83.21726\n",
      "epoca  90  loss  80.64216 mse test 82.44931\n",
      "epoca  91  loss  80.035034 mse test 81.66\n",
      "epoca  92  loss  79.87262 mse test 81.72412\n",
      "epoca  93  loss  79.700485 mse test 82.1226\n",
      "epoca  94  loss  79.36709 mse test 81.925644\n",
      "epoca  95  loss  79.17893 mse test 81.119545\n",
      "epoca  96  loss  78.91297 mse test 79.87832\n",
      "epoca  97  loss  78.60918 mse test 78.98214\n",
      "epoca  98  loss  78.49678 mse test 78.98672\n",
      "epoca  99  loss  78.18164 mse test 79.5684\n",
      "epoca  100  loss  78.09318 mse test 79.30275\n",
      "epoca  101  loss  77.984116 mse test 78.76228\n",
      "epoca  102  loss  77.851974 mse test 78.33328\n",
      "epoca  103  loss  77.896675 mse test 78.641685\n",
      "epoca  104  loss  77.77541 mse test 78.95771\n",
      "epoca  105  loss  77.77881 mse test 78.98672\n",
      "epoca  106  loss  77.56315 mse test 78.453896\n",
      "epoca  107  loss  77.20675 mse test 78.3516\n",
      "epoca  108  loss  77.03013 mse test 78.43099\n",
      "epoca  109  loss  76.36086 mse test 79.018776\n",
      "epoca  110  loss  75.91936 mse test 79.09206\n",
      "epoca  111  loss  75.42002 mse test 77.548546\n",
      "epoca  112  loss  74.87981 mse test 77.2371\n",
      "epoca  113  loss  74.353455 mse test 78.841675\n",
      "epoca  114  loss  74.34639 mse test 78.0829\n",
      "epoca  115  loss  74.17791 mse test 75.64779\n",
      "epoca  116  loss  74.204605 mse test 74.983665\n",
      "epoca  117  loss  74.06896 mse test 75.9287\n",
      "epoca  118  loss  74.05961 mse test 75.192825\n",
      "epoca  119  loss  74.00995 mse test 73.46916\n",
      "epoca  120  loss  74.00262 mse test 73.14702\n",
      "epoca  121  loss  73.93991 mse test 74.25084\n",
      "epoca  122  loss  73.89261 mse test 73.997406\n",
      "epoca  123  loss  73.88387 mse test 71.771454\n",
      "epoca  124  loss  73.85037 mse test 71.38519\n",
      "epoca  125  loss  73.80616 mse test 70.15314\n",
      "epoca  126  loss  73.47154 mse test 69.74244\n",
      "epoca  127  loss  73.06766 mse test 69.87527\n",
      "epoca  128  loss  72.799904 mse test 69.5455\n",
      "epoca  129  loss  72.59222 mse test 69.26916\n",
      "epoca  130  loss  72.465004 mse test 69.186714\n",
      "epoca  131  loss  72.34891 mse test 69.76534\n",
      "epoca  132  loss  72.32737 mse test 69.42489\n",
      "epoca  133  loss  72.30091 mse test 69.15923\n",
      "epoca  134  loss  72.25796 mse test 68.82183\n",
      "epoca  135  loss  72.15116 mse test 68.95466\n",
      "epoca  136  loss  71.73987 mse test 69.714966\n",
      "epoca  137  loss  71.70371 mse test 69.238625\n",
      "epoca  138  loss  71.01879 mse test 69.22183\n",
      "epoca  139  loss  71.08843 mse test 69.13481\n",
      "epoca  140  loss  69.83794 mse test 70.663055\n",
      "epoca  141  loss  70.124565 mse test 68.79282\n",
      "epoca  142  loss  69.37278 mse test 68.66611\n",
      "epoca  143  loss  69.527885 mse test 70.26611\n",
      "epoca  144  loss  69.29621 mse test 69.52564\n",
      "epoca  145  loss  69.18555 mse test 67.13786\n",
      "epoca  146  loss  69.28031 mse test 67.43557\n",
      "epoca  147  loss  69.02771 mse test 69.13023\n",
      "epoca  148  loss  69.276825 mse test 67.43252\n",
      "epoca  149  loss  68.95402 mse test 66.67222\n",
      "epoca  150  loss  68.82748 mse test 66.85237\n",
      "epoca  151  loss  68.619896 mse test 67.27985\n",
      "epoca  152  loss  68.49073 mse test 66.962296\n",
      "epoca  153  loss  68.24666 mse test 66.7455\n",
      "epoca  154  loss  67.97119 mse test 65.58825\n",
      "epoca  155  loss  67.76122 mse test 65.806564\n",
      "epoca  156  loss  67.36785 mse test 65.43864\n",
      "epoca  157  loss  67.12492 mse test 64.65847\n",
      "epoca  158  loss  66.96038 mse test 64.93787\n",
      "epoca  159  loss  66.75123 mse test 65.426414\n",
      "epoca  160  loss  66.756714 mse test 64.51955\n",
      "epoca  161  loss  66.66194 mse test 64.47679\n",
      "epoca  162  loss  66.62807 mse test 64.86\n",
      "epoca  163  loss  66.63152 mse test 64.34092\n",
      "epoca  164  loss  66.56569 mse test 64.05542\n",
      "epoca  165  loss  66.59767 mse test 64.67985\n",
      "epoca  166  loss  66.565384 mse test 64.618774\n",
      "epoca  167  loss  66.555336 mse test 64.03404\n",
      "epoca  168  loss  66.5611 mse test 64.26\n",
      "epoca  169  loss  66.515755 mse test 64.649315\n",
      "epoca  170  loss  66.51714 mse test 64.32259\n",
      "epoca  171  loss  66.48472 mse test 64.238625\n",
      "epoca  172  loss  66.47299 mse test 64.56381\n",
      "epoca  173  loss  66.4712 mse test 64.28596\n",
      "epoca  174  loss  66.44337 mse test 64.03404\n",
      "epoca  175  loss  66.44312 mse test 64.32718\n",
      "epoca  176  loss  66.41848 mse test 64.35924\n",
      "epoca  177  loss  66.40431 mse test 64.11344\n",
      "epoca  178  loss  66.39847 mse test 64.30581\n",
      "epoca  179  loss  66.377655 mse test 64.45695\n",
      "epoca  180  loss  66.372475 mse test 64.12565\n",
      "epoca  181  loss  66.352394 mse test 64.08748\n",
      "epoca  182  loss  66.33783 mse test 64.41115\n",
      "epoca  183  loss  66.32753 mse test 64.30885\n",
      "epoca  184  loss  66.30851 mse test 64.19741\n",
      "epoca  185  loss  66.300415 mse test 64.49664\n",
      "epoca  186  loss  66.28231 mse test 64.5287\n",
      "epoca  187  loss  66.2688 mse test 64.26764\n",
      "epoca  188  loss  66.25677 mse test 64.38978\n",
      "epoca  189  loss  66.241646 mse test 64.5287\n",
      "epoca  190  loss  66.23095 mse test 64.412674\n",
      "epoca  191  loss  66.21549 mse test 64.31344\n",
      "epoca  192  loss  66.20247 mse test 64.43558\n",
      "epoca  193  loss  66.19053 mse test 64.39588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoca  194  loss  66.1766 mse test 64.25848\n",
      "epoca  195  loss  66.164925 mse test 64.39741\n",
      "epoca  196  loss  66.15031 mse test 64.429474\n",
      "epoca  197  loss  66.13844 mse test 64.44321\n",
      "epoca  198  loss  66.12528 mse test 64.38978\n",
      "epoca  199  loss  66.11318 mse test 64.5226\n",
      "mse: 64.523\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Prepara os dados\n",
    "path = 'Auto2.csv'\n",
    "\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "\n",
    "# criar modelo\n",
    "model = MLP(7)\n",
    "# treina o modelo\n",
    "train_model(train_dl, model)\n",
    "# avalia\n",
    "mse = evaluate_model(test_dl, model)\n",
    "print('mse: %.3f' % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
