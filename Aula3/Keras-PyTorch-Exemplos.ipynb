{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from numpy import vstack\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn import MSELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Praticando Keras e PyTorch\n",
    "* Estrutura de Gradiente e Otimizador no PyTorch\n",
    "* CallBack - tensorboard para Keras e Pytorch\n",
    "* Classificação e Regressão no Keras\n",
    "* Classificação e Regressão no PyTorch\n",
    "* Inspeção de gradiente e Parâmetro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atualização de gradiente embutido no tensor\n",
    "* requires_grad inclui um parametro de gradiente no tensor. \n",
    "* Caso um tensor realize um cálculo com um tensor que possa o parametro requires_grad = true, ao chamar o método backward desse tensor, o parametro grad dos tensores associados serão atualizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad, retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 72., 162.])\n",
      "tensor([-24., -16.])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação com Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age       Class  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('pima-indians-diabetes.csv') \n",
    "print(df.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.226180</td>\n",
       "      <td>0.607510</td>\n",
       "      <td>0.566438</td>\n",
       "      <td>0.207439</td>\n",
       "      <td>0.094326</td>\n",
       "      <td>0.476790</td>\n",
       "      <td>0.194990</td>\n",
       "      <td>0.410381</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.198210</td>\n",
       "      <td>0.160666</td>\n",
       "      <td>0.158654</td>\n",
       "      <td>0.161134</td>\n",
       "      <td>0.136222</td>\n",
       "      <td>0.117499</td>\n",
       "      <td>0.136913</td>\n",
       "      <td>0.145188</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032231</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.497487</td>\n",
       "      <td>0.508197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406855</td>\n",
       "      <td>0.100723</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.587940</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.232323</td>\n",
       "      <td>0.036052</td>\n",
       "      <td>0.476900</td>\n",
       "      <td>0.153926</td>\n",
       "      <td>0.358025</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.704774</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.323232</td>\n",
       "      <td>0.150414</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.258781</td>\n",
       "      <td>0.506173</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      0.226180    0.607510       0.566438       0.207439    0.094326   \n",
       "std       0.198210    0.160666       0.158654       0.161134    0.136222   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       0.058824    0.497487       0.508197       0.000000    0.000000   \n",
       "50%       0.176471    0.587940       0.590164       0.232323    0.036052   \n",
       "75%       0.352941    0.704774       0.655738       0.323232    0.150414   \n",
       "max       1.000000    1.000000       1.000000       1.000000    1.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age       Class  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean     0.476790                  0.194990    0.410381    0.348958  \n",
       "std      0.117499                  0.136913    0.145188    0.476951  \n",
       "min      0.000000                  0.032231    0.259259    0.000000  \n",
       "25%      0.406855                  0.100723    0.296296    0.000000  \n",
       "50%      0.476900                  0.153926    0.358025    0.000000  \n",
       "75%      0.545455                  0.258781    0.506173    1.000000  \n",
       "max      1.000000                  1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = ['Class'] \n",
    "predictors = list(set(list(df.columns))-set(target_column))\n",
    "df[predictors] = df[predictors]/df[predictors].max()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(537, 8)\n",
      "(231, 8)\n"
     ]
    }
   ],
   "source": [
    "X = df[predictors].values\n",
    "y = df[target_column].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "count_classes = y_test.shape[1]\n",
    "print(count_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callback para tensorboard\n",
    "* tensorboard_callback\n",
    "* criar objeto callback\n",
    "* referenciar na chamada do fit  callbacks=[tensorboard_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "12/12 [==============================] - 1s 47ms/step - loss: 0.6537 - accuracy: 0.6800 - val_loss: 0.6712 - val_accuracy: 0.6358\n",
      "Epoch 2/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.6419 - accuracy: 0.6800 - val_loss: 0.6733 - val_accuracy: 0.6358\n",
      "Epoch 3/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.6272 - accuracy: 0.6800 - val_loss: 0.6531 - val_accuracy: 0.6358\n",
      "Epoch 4/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.6147 - accuracy: 0.6800 - val_loss: 0.6496 - val_accuracy: 0.6358\n",
      "Epoch 5/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.6102 - accuracy: 0.6800 - val_loss: 0.6412 - val_accuracy: 0.6420\n",
      "Epoch 6/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5867 - accuracy: 0.7147 - val_loss: 0.6254 - val_accuracy: 0.6605\n",
      "Epoch 7/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.5665 - accuracy: 0.7173 - val_loss: 0.6228 - val_accuracy: 0.6667\n",
      "Epoch 8/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5537 - accuracy: 0.7360 - val_loss: 0.6143 - val_accuracy: 0.6481\n",
      "Epoch 9/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.5397 - accuracy: 0.7573 - val_loss: 0.5928 - val_accuracy: 0.6728\n",
      "Epoch 10/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.5314 - accuracy: 0.7627 - val_loss: 0.6183 - val_accuracy: 0.6420\n",
      "Epoch 11/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.5160 - accuracy: 0.7493 - val_loss: 0.5758 - val_accuracy: 0.6914\n",
      "Epoch 12/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.5042 - accuracy: 0.7467 - val_loss: 0.5694 - val_accuracy: 0.6914\n",
      "Epoch 13/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.4945 - accuracy: 0.7627 - val_loss: 0.5646 - val_accuracy: 0.6852\n",
      "Epoch 14/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.4851 - accuracy: 0.7707 - val_loss: 0.5533 - val_accuracy: 0.7160\n",
      "Epoch 15/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.4836 - accuracy: 0.7600 - val_loss: 0.5776 - val_accuracy: 0.6667\n",
      "Epoch 16/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4689 - accuracy: 0.7840 - val_loss: 0.5369 - val_accuracy: 0.7284\n",
      "Epoch 17/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4764 - accuracy: 0.7680 - val_loss: 0.5329 - val_accuracy: 0.7099\n",
      "Epoch 18/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4609 - accuracy: 0.7813 - val_loss: 0.5392 - val_accuracy: 0.7284\n",
      "Epoch 19/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.4547 - accuracy: 0.8053 - val_loss: 0.5274 - val_accuracy: 0.7407\n",
      "Epoch 20/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4422 - accuracy: 0.8000 - val_loss: 0.5211 - val_accuracy: 0.7469\n",
      "Epoch 21/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4387 - accuracy: 0.7947 - val_loss: 0.5181 - val_accuracy: 0.7654\n",
      "Epoch 22/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4335 - accuracy: 0.7867 - val_loss: 0.5375 - val_accuracy: 0.7407\n",
      "Epoch 23/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.4371 - accuracy: 0.7973 - val_loss: 0.5268 - val_accuracy: 0.7593\n",
      "Epoch 24/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4318 - accuracy: 0.7840 - val_loss: 0.5144 - val_accuracy: 0.7654\n",
      "Epoch 25/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.4316 - accuracy: 0.7920 - val_loss: 0.5295 - val_accuracy: 0.7407\n",
      "Epoch 26/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4165 - accuracy: 0.7920 - val_loss: 0.5537 - val_accuracy: 0.7222\n",
      "Epoch 27/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4231 - accuracy: 0.8133 - val_loss: 0.5195 - val_accuracy: 0.7654\n",
      "Epoch 28/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4170 - accuracy: 0.7973 - val_loss: 0.5182 - val_accuracy: 0.7716\n",
      "Epoch 29/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4207 - accuracy: 0.7760 - val_loss: 0.5184 - val_accuracy: 0.7778\n",
      "Epoch 30/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4346 - accuracy: 0.8000 - val_loss: 0.5851 - val_accuracy: 0.6975\n",
      "Epoch 31/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4266 - accuracy: 0.7947 - val_loss: 0.5249 - val_accuracy: 0.7654\n",
      "Epoch 32/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4021 - accuracy: 0.8187 - val_loss: 0.5272 - val_accuracy: 0.7716\n",
      "Epoch 33/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4044 - accuracy: 0.8080 - val_loss: 0.5215 - val_accuracy: 0.7716\n",
      "Epoch 34/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3998 - accuracy: 0.8240 - val_loss: 0.5706 - val_accuracy: 0.7346\n",
      "Epoch 35/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.4100 - accuracy: 0.8027 - val_loss: 0.5343 - val_accuracy: 0.7593\n",
      "Epoch 36/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3907 - accuracy: 0.8187 - val_loss: 0.5577 - val_accuracy: 0.7531\n",
      "Epoch 37/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3974 - accuracy: 0.8133 - val_loss: 0.5317 - val_accuracy: 0.7531\n",
      "Epoch 38/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3918 - accuracy: 0.8347 - val_loss: 0.5433 - val_accuracy: 0.7778\n",
      "Epoch 39/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3894 - accuracy: 0.8240 - val_loss: 0.5396 - val_accuracy: 0.7840\n",
      "Epoch 40/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3960 - accuracy: 0.8267 - val_loss: 0.5394 - val_accuracy: 0.7901\n",
      "Epoch 41/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3884 - accuracy: 0.8133 - val_loss: 0.5364 - val_accuracy: 0.7654\n",
      "Epoch 42/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3766 - accuracy: 0.8347 - val_loss: 0.5746 - val_accuracy: 0.7407\n",
      "Epoch 43/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3923 - accuracy: 0.8133 - val_loss: 0.5412 - val_accuracy: 0.7407\n",
      "Epoch 44/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3802 - accuracy: 0.8293 - val_loss: 0.5440 - val_accuracy: 0.7531\n",
      "Epoch 45/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3774 - accuracy: 0.8320 - val_loss: 0.5455 - val_accuracy: 0.7840\n",
      "Epoch 46/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3799 - accuracy: 0.8187 - val_loss: 0.5502 - val_accuracy: 0.7593\n",
      "Epoch 47/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.3729 - accuracy: 0.8320 - val_loss: 0.5546 - val_accuracy: 0.7407\n",
      "Epoch 48/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3682 - accuracy: 0.8427 - val_loss: 0.5715 - val_accuracy: 0.7593\n",
      "Epoch 49/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3868 - accuracy: 0.8240 - val_loss: 0.5620 - val_accuracy: 0.7407\n",
      "Epoch 50/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3632 - accuracy: 0.8400 - val_loss: 0.5761 - val_accuracy: 0.7593\n",
      "Epoch 51/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3773 - accuracy: 0.8240 - val_loss: 0.5712 - val_accuracy: 0.7593\n",
      "Epoch 52/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3804 - accuracy: 0.8320 - val_loss: 0.5601 - val_accuracy: 0.7840\n",
      "Epoch 53/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3655 - accuracy: 0.8373 - val_loss: 0.5639 - val_accuracy: 0.7531\n",
      "Epoch 54/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3610 - accuracy: 0.8187 - val_loss: 0.5654 - val_accuracy: 0.7284\n",
      "Epoch 55/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3581 - accuracy: 0.8427 - val_loss: 0.5762 - val_accuracy: 0.7531\n",
      "Epoch 56/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3572 - accuracy: 0.8373 - val_loss: 0.5706 - val_accuracy: 0.7284\n",
      "Epoch 57/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3534 - accuracy: 0.8373 - val_loss: 0.5767 - val_accuracy: 0.7654\n",
      "Epoch 58/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3581 - accuracy: 0.8320 - val_loss: 0.6062 - val_accuracy: 0.7037\n",
      "Epoch 59/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3462 - accuracy: 0.8480 - val_loss: 0.5976 - val_accuracy: 0.7531\n",
      "Epoch 60/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3611 - accuracy: 0.8347 - val_loss: 0.5780 - val_accuracy: 0.7593\n",
      "Epoch 61/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3474 - accuracy: 0.8347 - val_loss: 0.5835 - val_accuracy: 0.7716\n",
      "Epoch 62/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3506 - accuracy: 0.8400 - val_loss: 0.5860 - val_accuracy: 0.7593\n",
      "Epoch 63/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3419 - accuracy: 0.8480 - val_loss: 0.5857 - val_accuracy: 0.7346\n",
      "Epoch 64/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3413 - accuracy: 0.8533 - val_loss: 0.5978 - val_accuracy: 0.7593\n",
      "Epoch 65/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3400 - accuracy: 0.8560 - val_loss: 0.5989 - val_accuracy: 0.7469\n",
      "Epoch 66/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3408 - accuracy: 0.8480 - val_loss: 0.5973 - val_accuracy: 0.7346\n",
      "Epoch 67/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3384 - accuracy: 0.8560 - val_loss: 0.6075 - val_accuracy: 0.7222\n",
      "Epoch 68/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3368 - accuracy: 0.8453 - val_loss: 0.6096 - val_accuracy: 0.7654\n",
      "Epoch 69/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3440 - accuracy: 0.8400 - val_loss: 0.6058 - val_accuracy: 0.7469\n",
      "Epoch 70/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3343 - accuracy: 0.8480 - val_loss: 0.6308 - val_accuracy: 0.7407\n",
      "Epoch 71/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3451 - accuracy: 0.8400 - val_loss: 0.6173 - val_accuracy: 0.7654\n",
      "Epoch 72/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.3345 - accuracy: 0.8533 - val_loss: 0.6099 - val_accuracy: 0.7407\n",
      "Epoch 73/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3359 - accuracy: 0.8533 - val_loss: 0.6382 - val_accuracy: 0.7222\n",
      "Epoch 74/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3349 - accuracy: 0.8560 - val_loss: 0.6232 - val_accuracy: 0.7284\n",
      "Epoch 75/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3403 - accuracy: 0.8480 - val_loss: 0.6457 - val_accuracy: 0.7469\n",
      "Epoch 76/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3312 - accuracy: 0.8533 - val_loss: 0.6191 - val_accuracy: 0.7407\n",
      "Epoch 77/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3361 - accuracy: 0.8533 - val_loss: 0.6662 - val_accuracy: 0.7099\n",
      "Epoch 78/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3573 - accuracy: 0.8427 - val_loss: 0.6328 - val_accuracy: 0.7346\n",
      "Epoch 79/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3407 - accuracy: 0.8533 - val_loss: 0.6386 - val_accuracy: 0.7531\n",
      "Epoch 80/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3272 - accuracy: 0.8587 - val_loss: 0.6394 - val_accuracy: 0.7407\n",
      "Epoch 81/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3273 - accuracy: 0.8587 - val_loss: 0.6376 - val_accuracy: 0.7284\n",
      "Epoch 82/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3331 - accuracy: 0.8533 - val_loss: 0.6838 - val_accuracy: 0.7099\n",
      "Epoch 83/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3458 - accuracy: 0.8453 - val_loss: 0.6225 - val_accuracy: 0.7593\n",
      "Epoch 84/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3155 - accuracy: 0.8640 - val_loss: 0.6384 - val_accuracy: 0.7654\n",
      "Epoch 85/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3217 - accuracy: 0.8773 - val_loss: 0.6421 - val_accuracy: 0.7222\n",
      "Epoch 86/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3146 - accuracy: 0.8587 - val_loss: 0.6480 - val_accuracy: 0.7346\n",
      "Epoch 87/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.3330 - accuracy: 0.8320 - val_loss: 0.6652 - val_accuracy: 0.7469\n",
      "Epoch 88/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3084 - accuracy: 0.8773 - val_loss: 0.6503 - val_accuracy: 0.7346\n",
      "Epoch 89/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3154 - accuracy: 0.8587 - val_loss: 0.6445 - val_accuracy: 0.7531\n",
      "Epoch 90/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3160 - accuracy: 0.8640 - val_loss: 0.6995 - val_accuracy: 0.7407\n",
      "Epoch 91/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3319 - accuracy: 0.8453 - val_loss: 0.6551 - val_accuracy: 0.7407\n",
      "Epoch 92/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3472 - accuracy: 0.8373 - val_loss: 0.6896 - val_accuracy: 0.7222\n",
      "Epoch 93/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3148 - accuracy: 0.8613 - val_loss: 0.6570 - val_accuracy: 0.7222\n",
      "Epoch 94/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3120 - accuracy: 0.8693 - val_loss: 0.6612 - val_accuracy: 0.7469\n",
      "Epoch 95/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3042 - accuracy: 0.8747 - val_loss: 0.6690 - val_accuracy: 0.7407\n",
      "Epoch 96/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2966 - accuracy: 0.8827 - val_loss: 0.6811 - val_accuracy: 0.7407\n",
      "Epoch 97/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3158 - accuracy: 0.8613 - val_loss: 0.6750 - val_accuracy: 0.7346\n",
      "Epoch 98/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3355 - accuracy: 0.8427 - val_loss: 0.6770 - val_accuracy: 0.7469\n",
      "Epoch 99/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3198 - accuracy: 0.8693 - val_loss: 0.6781 - val_accuracy: 0.7531\n",
      "Epoch 100/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.3091 - accuracy: 0.8587 - val_loss: 0.6722 - val_accuracy: 0.7469\n",
      "Epoch 101/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2976 - accuracy: 0.8747 - val_loss: 0.7017 - val_accuracy: 0.7346\n",
      "Epoch 102/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3347 - accuracy: 0.8507 - val_loss: 0.6645 - val_accuracy: 0.7407\n",
      "Epoch 103/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3184 - accuracy: 0.8613 - val_loss: 0.7019 - val_accuracy: 0.7284\n",
      "Epoch 104/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2965 - accuracy: 0.8693 - val_loss: 0.6910 - val_accuracy: 0.7346\n",
      "Epoch 105/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2883 - accuracy: 0.8827 - val_loss: 0.6645 - val_accuracy: 0.7531\n",
      "Epoch 106/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2923 - accuracy: 0.8853 - val_loss: 0.7005 - val_accuracy: 0.7407\n",
      "Epoch 107/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2844 - accuracy: 0.8800 - val_loss: 0.7021 - val_accuracy: 0.7346\n",
      "Epoch 108/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2931 - accuracy: 0.8693 - val_loss: 0.6908 - val_accuracy: 0.7407\n",
      "Epoch 109/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2986 - accuracy: 0.8800 - val_loss: 0.7081 - val_accuracy: 0.7407\n",
      "Epoch 110/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.3009 - accuracy: 0.8667 - val_loss: 0.7057 - val_accuracy: 0.7346\n",
      "Epoch 111/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2813 - accuracy: 0.8853 - val_loss: 0.6988 - val_accuracy: 0.7346\n",
      "Epoch 112/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.2800 - accuracy: 0.8800 - val_loss: 0.7434 - val_accuracy: 0.7284\n",
      "Epoch 113/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2979 - accuracy: 0.8613 - val_loss: 0.7016 - val_accuracy: 0.7284\n",
      "Epoch 114/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2770 - accuracy: 0.8853 - val_loss: 0.7044 - val_accuracy: 0.7407\n",
      "Epoch 115/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2815 - accuracy: 0.8853 - val_loss: 0.7056 - val_accuracy: 0.7284\n",
      "Epoch 116/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2793 - accuracy: 0.8693 - val_loss: 0.7237 - val_accuracy: 0.7407\n",
      "Epoch 117/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2994 - accuracy: 0.8773 - val_loss: 0.7225 - val_accuracy: 0.7407\n",
      "Epoch 118/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2729 - accuracy: 0.8880 - val_loss: 0.7249 - val_accuracy: 0.7469\n",
      "Epoch 119/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2737 - accuracy: 0.8853 - val_loss: 0.7164 - val_accuracy: 0.7469\n",
      "Epoch 120/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2679 - accuracy: 0.8880 - val_loss: 0.7226 - val_accuracy: 0.7407\n",
      "Epoch 121/200\n",
      "12/12 [==============================] - 0s 8ms/step - loss: 0.2625 - accuracy: 0.8933 - val_loss: 0.7283 - val_accuracy: 0.7469\n",
      "Epoch 122/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2904 - accuracy: 0.8880 - val_loss: 0.7984 - val_accuracy: 0.7099\n",
      "Epoch 123/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2901 - accuracy: 0.8533 - val_loss: 0.7166 - val_accuracy: 0.7407\n",
      "Epoch 124/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2666 - accuracy: 0.8773 - val_loss: 0.7328 - val_accuracy: 0.7284\n",
      "Epoch 125/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2780 - accuracy: 0.8720 - val_loss: 0.7587 - val_accuracy: 0.7346\n",
      "Epoch 126/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2605 - accuracy: 0.8907 - val_loss: 0.7350 - val_accuracy: 0.7346\n",
      "Epoch 127/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2574 - accuracy: 0.8880 - val_loss: 0.7589 - val_accuracy: 0.7407\n",
      "Epoch 128/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2585 - accuracy: 0.8907 - val_loss: 0.7481 - val_accuracy: 0.7346\n",
      "Epoch 129/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2580 - accuracy: 0.8827 - val_loss: 0.7470 - val_accuracy: 0.7407\n",
      "Epoch 130/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2585 - accuracy: 0.8933 - val_loss: 0.7796 - val_accuracy: 0.7346\n",
      "Epoch 131/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2693 - accuracy: 0.8880 - val_loss: 0.7845 - val_accuracy: 0.7346\n",
      "Epoch 132/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2673 - accuracy: 0.8880 - val_loss: 0.7440 - val_accuracy: 0.7469\n",
      "Epoch 133/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2560 - accuracy: 0.8987 - val_loss: 0.7604 - val_accuracy: 0.7407\n",
      "Epoch 134/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2571 - accuracy: 0.8880 - val_loss: 0.7684 - val_accuracy: 0.7284\n",
      "Epoch 135/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2558 - accuracy: 0.8880 - val_loss: 0.7978 - val_accuracy: 0.7284\n",
      "Epoch 136/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2801 - accuracy: 0.8693 - val_loss: 0.7693 - val_accuracy: 0.7407\n",
      "Epoch 137/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2536 - accuracy: 0.8960 - val_loss: 0.7788 - val_accuracy: 0.7284\n",
      "Epoch 138/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2474 - accuracy: 0.8933 - val_loss: 0.7763 - val_accuracy: 0.7469\n",
      "Epoch 139/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2464 - accuracy: 0.8960 - val_loss: 0.7804 - val_accuracy: 0.7407\n",
      "Epoch 140/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2367 - accuracy: 0.9013 - val_loss: 0.7855 - val_accuracy: 0.7346\n",
      "Epoch 141/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2359 - accuracy: 0.8933 - val_loss: 0.8079 - val_accuracy: 0.7469\n",
      "Epoch 142/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2424 - accuracy: 0.8933 - val_loss: 0.8127 - val_accuracy: 0.7346\n",
      "Epoch 143/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2611 - accuracy: 0.8800 - val_loss: 0.7924 - val_accuracy: 0.7222\n",
      "Epoch 144/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2500 - accuracy: 0.8960 - val_loss: 0.7879 - val_accuracy: 0.7407\n",
      "Epoch 145/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2516 - accuracy: 0.8880 - val_loss: 0.7932 - val_accuracy: 0.7160\n",
      "Epoch 146/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2507 - accuracy: 0.8880 - val_loss: 0.7949 - val_accuracy: 0.7407\n",
      "Epoch 147/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2331 - accuracy: 0.9067 - val_loss: 0.8041 - val_accuracy: 0.7407\n",
      "Epoch 148/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2318 - accuracy: 0.9067 - val_loss: 0.8108 - val_accuracy: 0.7346\n",
      "Epoch 149/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2420 - accuracy: 0.9120 - val_loss: 0.8247 - val_accuracy: 0.7469\n",
      "Epoch 150/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2789 - accuracy: 0.8853 - val_loss: 0.8102 - val_accuracy: 0.7469\n",
      "Epoch 151/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2461 - accuracy: 0.9013 - val_loss: 0.8648 - val_accuracy: 0.7160\n",
      "Epoch 152/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2465 - accuracy: 0.8853 - val_loss: 0.8065 - val_accuracy: 0.7346\n",
      "Epoch 153/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2418 - accuracy: 0.8907 - val_loss: 0.8042 - val_accuracy: 0.7284\n",
      "Epoch 154/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2472 - accuracy: 0.8880 - val_loss: 0.8382 - val_accuracy: 0.7160\n",
      "Epoch 155/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2413 - accuracy: 0.9093 - val_loss: 0.8226 - val_accuracy: 0.7160\n",
      "Epoch 156/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2326 - accuracy: 0.9120 - val_loss: 0.8408 - val_accuracy: 0.7284\n",
      "Epoch 157/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2288 - accuracy: 0.8960 - val_loss: 0.8464 - val_accuracy: 0.7284\n",
      "Epoch 158/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2458 - accuracy: 0.8880 - val_loss: 0.8210 - val_accuracy: 0.7407\n",
      "Epoch 159/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2186 - accuracy: 0.9253 - val_loss: 0.8296 - val_accuracy: 0.7407\n",
      "Epoch 160/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2205 - accuracy: 0.9120 - val_loss: 0.8417 - val_accuracy: 0.7407\n",
      "Epoch 161/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2245 - accuracy: 0.9147 - val_loss: 0.8489 - val_accuracy: 0.7407\n",
      "Epoch 162/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2238 - accuracy: 0.9120 - val_loss: 0.8608 - val_accuracy: 0.7160\n",
      "Epoch 163/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2190 - accuracy: 0.9093 - val_loss: 0.8539 - val_accuracy: 0.7407\n",
      "Epoch 164/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2161 - accuracy: 0.9200 - val_loss: 0.8521 - val_accuracy: 0.7407\n",
      "Epoch 165/200\n",
      "12/12 [==============================] - 0s 22ms/step - loss: 0.2268 - accuracy: 0.8933 - val_loss: 0.8945 - val_accuracy: 0.7160\n",
      "Epoch 166/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2275 - accuracy: 0.9120 - val_loss: 0.8582 - val_accuracy: 0.7346\n",
      "Epoch 167/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.2227 - accuracy: 0.9067 - val_loss: 0.8466 - val_accuracy: 0.7469\n",
      "Epoch 168/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2144 - accuracy: 0.9120 - val_loss: 0.8917 - val_accuracy: 0.7284\n",
      "Epoch 169/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2170 - accuracy: 0.9173 - val_loss: 0.8729 - val_accuracy: 0.7407\n",
      "Epoch 170/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2212 - accuracy: 0.9147 - val_loss: 0.8903 - val_accuracy: 0.7346\n",
      "Epoch 171/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2091 - accuracy: 0.9200 - val_loss: 0.8777 - val_accuracy: 0.7407\n",
      "Epoch 172/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2178 - accuracy: 0.9067 - val_loss: 0.9127 - val_accuracy: 0.7222\n",
      "Epoch 173/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2135 - accuracy: 0.9120 - val_loss: 0.8985 - val_accuracy: 0.7222\n",
      "Epoch 174/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2016 - accuracy: 0.9333 - val_loss: 0.8856 - val_accuracy: 0.7346\n",
      "Epoch 175/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1985 - accuracy: 0.9307 - val_loss: 0.8996 - val_accuracy: 0.7346\n",
      "Epoch 176/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2006 - accuracy: 0.9387 - val_loss: 0.9063 - val_accuracy: 0.7407\n",
      "Epoch 177/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2042 - accuracy: 0.9040 - val_loss: 0.8925 - val_accuracy: 0.7284\n",
      "Epoch 178/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1975 - accuracy: 0.9280 - val_loss: 0.9155 - val_accuracy: 0.7407\n",
      "Epoch 179/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.2009 - accuracy: 0.9307 - val_loss: 0.9223 - val_accuracy: 0.7284\n",
      "Epoch 180/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2096 - accuracy: 0.9173 - val_loss: 0.9179 - val_accuracy: 0.7346\n",
      "Epoch 181/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.2013 - accuracy: 0.9333 - val_loss: 0.9207 - val_accuracy: 0.7346\n",
      "Epoch 182/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1854 - accuracy: 0.9387 - val_loss: 0.9384 - val_accuracy: 0.7284\n",
      "Epoch 183/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1890 - accuracy: 0.9253 - val_loss: 0.9523 - val_accuracy: 0.7222\n",
      "Epoch 184/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1916 - accuracy: 0.9307 - val_loss: 0.9456 - val_accuracy: 0.7346\n",
      "Epoch 185/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1924 - accuracy: 0.9280 - val_loss: 0.9355 - val_accuracy: 0.7222\n",
      "Epoch 186/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1821 - accuracy: 0.9307 - val_loss: 0.9617 - val_accuracy: 0.7284\n",
      "Epoch 187/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1891 - accuracy: 0.9253 - val_loss: 0.9494 - val_accuracy: 0.7222\n",
      "Epoch 188/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1876 - accuracy: 0.9307 - val_loss: 0.9594 - val_accuracy: 0.7346\n",
      "Epoch 189/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1893 - accuracy: 0.9307 - val_loss: 0.9750 - val_accuracy: 0.7346\n",
      "Epoch 190/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1902 - accuracy: 0.9360 - val_loss: 0.9619 - val_accuracy: 0.7407\n",
      "Epoch 191/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1948 - accuracy: 0.9387 - val_loss: 0.9928 - val_accuracy: 0.7346\n",
      "Epoch 192/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1939 - accuracy: 0.9280 - val_loss: 0.9786 - val_accuracy: 0.7407\n",
      "Epoch 193/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1755 - accuracy: 0.9387 - val_loss: 0.9822 - val_accuracy: 0.7407\n",
      "Epoch 194/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1831 - accuracy: 0.9333 - val_loss: 1.0416 - val_accuracy: 0.7037\n",
      "Epoch 195/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1830 - accuracy: 0.9360 - val_loss: 0.9992 - val_accuracy: 0.7284\n",
      "Epoch 196/200\n",
      "12/12 [==============================] - 0s 11ms/step - loss: 0.1800 - accuracy: 0.9280 - val_loss: 1.0019 - val_accuracy: 0.7284\n",
      "Epoch 197/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1837 - accuracy: 0.9307 - val_loss: 1.0030 - val_accuracy: 0.7346\n",
      "Epoch 198/200\n",
      "12/12 [==============================] - 0s 9ms/step - loss: 0.1884 - accuracy: 0.9147 - val_loss: 1.0394 - val_accuracy: 0.7099\n",
      "Epoch 199/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1727 - accuracy: 0.9360 - val_loss: 1.0234 - val_accuracy: 0.7222\n",
      "Epoch 200/200\n",
      "12/12 [==============================] - 0s 10ms/step - loss: 0.1921 - accuracy: 0.9173 - val_loss: 1.0670 - val_accuracy: 0.7037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc0305e6a30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logDir = \"/home/silvio/tb/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logDir)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=200, callbacks=[tensorboard_callback], validation_split=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão com Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.515869</td>\n",
       "      <td>5.458438</td>\n",
       "      <td>193.532746</td>\n",
       "      <td>104.209068</td>\n",
       "      <td>2970.261965</td>\n",
       "      <td>15.555668</td>\n",
       "      <td>75.994962</td>\n",
       "      <td>1.574307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.825804</td>\n",
       "      <td>1.701577</td>\n",
       "      <td>104.379583</td>\n",
       "      <td>38.338262</td>\n",
       "      <td>847.904119</td>\n",
       "      <td>2.749995</td>\n",
       "      <td>3.690005</td>\n",
       "      <td>0.802549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>1613.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>2223.000000</td>\n",
       "      <td>13.800000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>2800.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>262.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>3609.000000</td>\n",
       "      <td>17.100000</td>\n",
       "      <td>79.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>46.600000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>455.000000</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>5140.000000</td>\n",
       "      <td>24.800000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mpg   cylinders  displacement  horsepower       weight  \\\n",
       "count  397.000000  397.000000    397.000000  397.000000   397.000000   \n",
       "mean    23.515869    5.458438    193.532746  104.209068  2970.261965   \n",
       "std      7.825804    1.701577    104.379583   38.338262   847.904119   \n",
       "min      9.000000    3.000000     68.000000   46.000000  1613.000000   \n",
       "25%     17.500000    4.000000    104.000000   75.000000  2223.000000   \n",
       "50%     23.000000    4.000000    146.000000   93.000000  2800.000000   \n",
       "75%     29.000000    8.000000    262.000000  125.000000  3609.000000   \n",
       "max     46.600000    8.000000    455.000000  230.000000  5140.000000   \n",
       "\n",
       "       acceleration        year      origin  \n",
       "count    397.000000  397.000000  397.000000  \n",
       "mean      15.555668   75.994962    1.574307  \n",
       "std        2.749995    3.690005    0.802549  \n",
       "min        8.000000   70.000000    1.000000  \n",
       "25%       13.800000   73.000000    1.000000  \n",
       "50%       15.500000   76.000000    1.000000  \n",
       "75%       17.100000   79.000000    2.000000  \n",
       "max       24.800000   82.000000    3.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Auto2.csv') \n",
    "print(df.shape)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>originL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86</td>\n",
       "      <td>2790</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52</td>\n",
       "      <td>2130</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84</td>\n",
       "      <td>2295</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79</td>\n",
       "      <td>2625</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82</td>\n",
       "      <td>2720</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mpg  cylinders  displacement  horsepower  weight  acceleration  year  \\\n",
       "0    18.0          8         307.0         130    3504          12.0    70   \n",
       "1    15.0          8         350.0         165    3693          11.5    70   \n",
       "2    18.0          8         318.0         150    3436          11.0    70   \n",
       "3    16.0          8         304.0         150    3433          12.0    70   \n",
       "4    17.0          8         302.0         140    3449          10.5    70   \n",
       "..    ...        ...           ...         ...     ...           ...   ...   \n",
       "392  27.0          4         140.0          86    2790          15.6    82   \n",
       "393  44.0          4          97.0          52    2130          24.6    82   \n",
       "394  32.0          4         135.0          84    2295          11.6    82   \n",
       "395  28.0          4         120.0          79    2625          18.6    82   \n",
       "396  31.0          4         119.0          82    2720          19.4    82   \n",
       "\n",
       "     originL  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  \n",
       "..       ...  \n",
       "392        0  \n",
       "393        1  \n",
       "394        0  \n",
       "395        0  \n",
       "396        0  \n",
       "\n",
       "[397 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "df['originL']= le.fit_transform(df['origin'])\n",
    "\n",
    "df=df.drop(columns = ['name','origin'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>year</th>\n",
       "      <th>originL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>397.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>23.515869</td>\n",
       "      <td>0.682305</td>\n",
       "      <td>0.425347</td>\n",
       "      <td>0.453083</td>\n",
       "      <td>0.577872</td>\n",
       "      <td>0.627245</td>\n",
       "      <td>0.926768</td>\n",
       "      <td>0.287154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.825804</td>\n",
       "      <td>0.212697</td>\n",
       "      <td>0.229406</td>\n",
       "      <td>0.166688</td>\n",
       "      <td>0.164962</td>\n",
       "      <td>0.110887</td>\n",
       "      <td>0.045000</td>\n",
       "      <td>0.401275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.149451</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.313813</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.853659</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.432490</td>\n",
       "      <td>0.556452</td>\n",
       "      <td>0.890244</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.320879</td>\n",
       "      <td>0.404348</td>\n",
       "      <td>0.544747</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>29.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.575824</td>\n",
       "      <td>0.543478</td>\n",
       "      <td>0.702140</td>\n",
       "      <td>0.689516</td>\n",
       "      <td>0.963415</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>46.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              mpg   cylinders  displacement  horsepower      weight  \\\n",
       "count  397.000000  397.000000    397.000000  397.000000  397.000000   \n",
       "mean    23.515869    0.682305      0.425347    0.453083    0.577872   \n",
       "std      7.825804    0.212697      0.229406    0.166688    0.164962   \n",
       "min      9.000000    0.375000      0.149451    0.200000    0.313813   \n",
       "25%     17.500000    0.500000      0.228571    0.326087    0.432490   \n",
       "50%     23.000000    0.500000      0.320879    0.404348    0.544747   \n",
       "75%     29.000000    1.000000      0.575824    0.543478    0.702140   \n",
       "max     46.600000    1.000000      1.000000    1.000000    1.000000   \n",
       "\n",
       "       acceleration        year     originL  \n",
       "count    397.000000  397.000000  397.000000  \n",
       "mean       0.627245    0.926768    0.287154  \n",
       "std        0.110887    0.045000    0.401275  \n",
       "min        0.322581    0.853659    0.000000  \n",
       "25%        0.556452    0.890244    0.000000  \n",
       "50%        0.625000    0.926829    0.000000  \n",
       "75%        0.689516    0.963415    0.500000  \n",
       "max        1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_column = ['mpg'] \n",
    "predictors = list(set(list(df.columns))-set(target_column))\n",
    "df[predictors] = df[predictors]/df[predictors].max()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(277, 7)\n",
      "(120, 7)\n"
     ]
    }
   ],
   "source": [
    "X = df[predictors].values\n",
    "y = df[target_column].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.RMSprop()\n",
    "\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['mae', 'mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 2/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 3/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 4/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 5/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 6/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 7/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 8/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 9/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 10/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 11/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 12/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 13/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 14/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 15/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 16/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 17/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 18/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 19/200\n",
      "7/7 [==============================] - 0s 14ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 20/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 21/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 22/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 23/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 24/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 25/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 26/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 27/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 28/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 29/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 30/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 31/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 32/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 33/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 34/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 35/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 36/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 37/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 38/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 39/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 40/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 41/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 42/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 43/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 44/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 45/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 46/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 47/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 48/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 49/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 50/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 51/200\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 52/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 53/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 54/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 55/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 56/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 57/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 58/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 59/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 60/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 61/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 62/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 63/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 64/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 65/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 66/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 67/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 68/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 69/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 70/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 71/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 72/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 73/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 74/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 75/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 76/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 77/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 78/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 79/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 80/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 81/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 82/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 83/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 84/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 85/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 86/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 87/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 88/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 89/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 90/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 91/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 92/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 93/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 94/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 95/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 14ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 96/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 97/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 98/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 99/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 100/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 101/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 102/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 103/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 104/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 105/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 106/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 107/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 108/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 109/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 110/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 111/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 112/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 113/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 114/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 115/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 116/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 117/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 118/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 119/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 120/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 121/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 122/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 123/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 124/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 125/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 126/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 127/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 128/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 129/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 130/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 131/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 132/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 133/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 134/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 135/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 136/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 137/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 138/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 139/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 140/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 141/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 142/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 143/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 144/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 145/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 146/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 147/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 148/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 149/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 150/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 151/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 152/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 153/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 154/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 155/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 156/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 157/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 158/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 159/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 160/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 161/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 162/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 163/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 164/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 165/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 166/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 167/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 168/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 169/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 170/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 171/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 172/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 173/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 174/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 175/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 176/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 177/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 178/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 179/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 180/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 181/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 182/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 183/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 184/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 185/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 186/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 187/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 188/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 189/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 190/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 191/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 192/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 193/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 194/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 195/200\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 196/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 197/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 198/200\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 577.0961 - mae: 22.7751 - mse: 577.0961 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 199/200\n",
      "7/7 [==============================] - 0s 13ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n",
      "Epoch 200/200\n",
      "7/7 [==============================] - 0s 12ms/step - loss: 577.0962 - mae: 22.7751 - mse: 577.0962 - val_loss: 692.7633 - val_mae: 25.0881 - val_mse: 692.7633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc0300bd910>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logDir = \"/home/silvio/tb/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logDir)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=200, callbacks=[tensorboard_callback], validation_split=0.3)\n",
    "#model.fit(X_train, y_train, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação com Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callback para tensorboard - Pytorch\n",
    "* criar objeto de escrita no tensorboard\n",
    "    * writerTest = SummaryWriter(logDir)\n",
    "* adicionar entrada no tensorboard (ex: uma vez por época)\n",
    "    * writerTest.add_scalar('epoch_accuracy', acctest, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiabetesDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        self.X = df.values[:, :-1]\n",
    "        self.y = df.values[:, -1]\n",
    "        \n",
    "        self.X = self.X.astype('float32')\n",
    "\n",
    "\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    " \n",
    "    # quantas linhas tem no dataset?\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # obtem uma linha do dataset\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    # retorna base para treino e teste\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        return random_split(self, [train_size, test_size])\n",
    "    \n",
    "def prepare_data(path):\n",
    "    # Carrega Dataset\n",
    "    dataset = DiabetesDataset(path)\n",
    "    # realiza split\n",
    "    train, test = dataset.get_splits()\n",
    "    # monta data loaders\n",
    "    train_dl = DataLoader(train, batch_size=1024, shuffle=True)\n",
    "    #test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    \n",
    "    return train_dl, test_dl\n",
    "\n",
    "class MLP(Module):\n",
    "    # Elementos do modelo\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # camada de entrada\n",
    "        self.hidden1 = Linear(n_inputs, 64)\n",
    "        # Inicialização da camada de entrada\n",
    "        #kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        # Ativação da camada de entrada\n",
    "        self.act1 = ReLU()\n",
    "        # segunda camada , entrada tem que ser do mesmo tamanho da saida da camada 1\n",
    "        self.hidden2 = Linear(64, 8)\n",
    "        # Inicialização da camada\n",
    "        #kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        # Ativação da camada\n",
    "        self.act2 = ReLU()\n",
    "        self.hidden3 = Linear(8, 8)\n",
    "        self.act3 = ReLU()\n",
    "        \n",
    "        # camada de saída\n",
    "        self.hidden4 = Linear(8, 1)\n",
    "        # Inicialização da camada\n",
    "        #xavier_uniform_(self.hidden3.weight)\n",
    "        # Ativação da camada\n",
    "        self.act4 = Sigmoid()\n",
    " \n",
    "    # propagação da entrada pelas camadas\n",
    "    def forward(self, X):\n",
    "        # entrada para primeira camada escondida\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        # segunda camada escondida\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # terceira camada escondida\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        # camada de saida\n",
    "        X = self.hidden4(X)\n",
    "        X = self.act4(X)\n",
    "        return X\n",
    "\n",
    "\n",
    "\n",
    "logDir = \"/home/silvio/tb/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/train/\"\n",
    "\n",
    "writerTrain = SummaryWriter(logDir)\n",
    "logDir = \"/home/silvio/tb/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/test/\"\n",
    "writerTest = SummaryWriter(logDir)\n",
    "\n",
    "def train_model(train_dl, model):\n",
    "    # define loss\n",
    "    criterion = BCELoss()\n",
    "    # define otimizador\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # loop por épocas\n",
    "    for epoch in range(200):\n",
    "        # Loop em conjunto de mini-batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # zera os gradientes do batches\n",
    "            optimizer.zero_grad()\n",
    "            # predição do batch\n",
    "            yhat = model(inputs)\n",
    "            # calcula loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            \n",
    "            \n",
    "            #print(yhat.detach().numpy(), targets.detach().numpy())\n",
    "            #print(accuracy_score(yhat.detach().numpy(), targets.detach().numpy()))\n",
    "            \n",
    "            #print(type(yhat), type(targets))\n",
    "            #print()\n",
    "            \n",
    "            #writer.add_scalar('Loss ', loss.detach().numpy(),epoch)    \n",
    "            \n",
    "            # Retroprapagando erros \n",
    "            loss.backward()\n",
    "            # atualiza pesos (otimização )\n",
    "            optimizer.step()\n",
    "            #print(loss)\n",
    "            \n",
    "            acctest = evaluate_model(test_dl, model)\n",
    "            acctrain = evaluate_model(train_dl, model)\n",
    "            #writer.add_scalar('epoch_loss', loss.detach().numpy(), epoch)\n",
    "            writerTrain.add_scalar('epoch_accuracy', acctrain, epoch)\n",
    "            #writer.add_scalar('epoch_accuracy', acctest, epoch)\n",
    "            writerTrain.add_scalar('epoch_loss', loss.detach().numpy(), epoch)\n",
    "            writerTest.add_scalar('epoch_accuracy', acctest, epoch)\n",
    "            \n",
    "            print(\"epoca \" , epoch, \" loss \", loss.detach().numpy() , \" acctest \", acctest,\" acctrain \", acctrain)\n",
    "\n",
    "\n",
    "def evaluate_model(test_dl, model):\n",
    "    # Cria lista de preditos e reais\n",
    "    predictions, actuals = list(), list()\n",
    "\n",
    "    # percorre lista do dataloader de test\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # realiza predição\n",
    "        yhat = model(inputs)\n",
    "        # cria numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round valores da classe\n",
    "        yhat = yhat.round()\n",
    "        # armazena\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "        \n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # usa sklearn para calcular acurácia\n",
    "    acc = accuracy_score(actuals, predictions)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515 253\n",
      "epoca  0  loss  0.6901595  acctest  0.541501976284585  acctrain  0.6038834951456311\n",
      "epoca  1  loss  0.6896743  acctest  0.5889328063241107  acctrain  0.6194174757281553\n",
      "epoca  2  loss  0.6887585  acctest  0.6324110671936759  acctrain  0.6349514563106796\n",
      "epoca  3  loss  0.68746644  acctest  0.6600790513833992  acctrain  0.6349514563106796\n",
      "epoca  4  loss  0.6858494  acctest  0.6561264822134387  acctrain  0.6388349514563106\n",
      "epoca  5  loss  0.68395394  acctest  0.6798418972332015  acctrain  0.6679611650485436\n",
      "epoca  6  loss  0.6818271  acctest  0.6758893280632411  acctrain  0.6776699029126214\n",
      "epoca  7  loss  0.6795147  acctest  0.6719367588932806  acctrain  0.6718446601941748\n",
      "epoca  8  loss  0.6770553  acctest  0.6482213438735178  acctrain  0.6679611650485436\n",
      "epoca  9  loss  0.6744856  acctest  0.6482213438735178  acctrain  0.6601941747572816\n",
      "epoca  10  loss  0.6718457  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  11  loss  0.6691664  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  12  loss  0.6664817  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  13  loss  0.66381  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  14  loss  0.6611682  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  15  loss  0.65857065  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  16  loss  0.6560336  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  17  loss  0.6535699  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  18  loss  0.6511841  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  19  loss  0.64888424  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  20  loss  0.6466761  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  21  loss  0.64455885  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  22  loss  0.6425365  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  23  loss  0.6406071  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  24  loss  0.6387699  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  25  loss  0.63701946  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  26  loss  0.63535714  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  27  loss  0.6337825  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  28  loss  0.6322918  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  29  loss  0.630882  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  30  loss  0.6295468  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  31  loss  0.62827814  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  32  loss  0.6270735  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  33  loss  0.6259211  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  34  loss  0.6248096  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  35  loss  0.6237392  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  36  loss  0.6227066  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  37  loss  0.62170684  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  38  loss  0.6207372  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  39  loss  0.6198001  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  40  loss  0.6188819  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  41  loss  0.61797684  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  42  loss  0.61708224  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  43  loss  0.6161923  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  44  loss  0.61530375  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  45  loss  0.6144206  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  46  loss  0.6135344  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  47  loss  0.6126477  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  48  loss  0.6117537  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  49  loss  0.6108452  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  50  loss  0.60994035  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  51  loss  0.60903084  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  52  loss  0.60812205  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  53  loss  0.60720193  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  54  loss  0.6062746  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  55  loss  0.6053268  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  56  loss  0.60435855  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  57  loss  0.6033714  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  58  loss  0.6023714  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  59  loss  0.60136175  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  60  loss  0.60033464  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  61  loss  0.5992857  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  62  loss  0.59821725  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  63  loss  0.5971239  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  64  loss  0.5960235  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  65  loss  0.59491116  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  66  loss  0.5937658  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  67  loss  0.59258485  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  68  loss  0.59137076  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  69  loss  0.5901315  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  70  loss  0.58886963  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  71  loss  0.5875901  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  72  loss  0.58629143  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  73  loss  0.58496034  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  74  loss  0.58360267  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  75  loss  0.582208  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  76  loss  0.58079505  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  77  loss  0.5793529  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  78  loss  0.5778922  acctest  0.6363636363636364  acctrain  0.658252427184466\n",
      "epoca  79  loss  0.57641137  acctest  0.6363636363636364  acctrain  0.6601941747572816\n",
      "epoca  80  loss  0.5749138  acctest  0.6363636363636364  acctrain  0.6601941747572816\n",
      "epoca  81  loss  0.5733898  acctest  0.6363636363636364  acctrain  0.6601941747572816\n",
      "epoca  82  loss  0.5718409  acctest  0.6363636363636364  acctrain  0.6601941747572816\n",
      "epoca  83  loss  0.5702636  acctest  0.6363636363636364  acctrain  0.6621359223300971\n",
      "epoca  84  loss  0.5686596  acctest  0.6363636363636364  acctrain  0.6621359223300971\n",
      "epoca  85  loss  0.5670344  acctest  0.6363636363636364  acctrain  0.6621359223300971\n",
      "epoca  86  loss  0.5653991  acctest  0.6363636363636364  acctrain  0.6621359223300971\n",
      "epoca  87  loss  0.56375414  acctest  0.6403162055335968  acctrain  0.6621359223300971\n",
      "epoca  88  loss  0.56211  acctest  0.6442687747035574  acctrain  0.6621359223300971\n",
      "epoca  89  loss  0.56046164  acctest  0.6442687747035574  acctrain  0.6679611650485436\n",
      "epoca  90  loss  0.5588091  acctest  0.6482213438735178  acctrain  0.6699029126213593\n",
      "epoca  91  loss  0.5571459  acctest  0.6521739130434783  acctrain  0.6679611650485436\n",
      "epoca  92  loss  0.5554656  acctest  0.6521739130434783  acctrain  0.6718446601941748\n",
      "epoca  93  loss  0.5537656  acctest  0.6561264822134387  acctrain  0.6718446601941748\n",
      "epoca  94  loss  0.55206704  acctest  0.6561264822134387  acctrain  0.6757281553398058\n",
      "epoca  95  loss  0.550359  acctest  0.6600790513833992  acctrain  0.6757281553398058\n",
      "epoca  96  loss  0.5486689  acctest  0.6640316205533597  acctrain  0.6776699029126214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoca  97  loss  0.5469894  acctest  0.6679841897233202  acctrain  0.6796116504854369\n",
      "epoca  98  loss  0.54533064  acctest  0.6679841897233202  acctrain  0.6815533980582524\n",
      "epoca  99  loss  0.5436632  acctest  0.6679841897233202  acctrain  0.6873786407766991\n",
      "epoca  100  loss  0.54202205  acctest  0.6758893280632411  acctrain  0.6912621359223301\n",
      "epoca  101  loss  0.5403822  acctest  0.6758893280632411  acctrain  0.6990291262135923\n",
      "epoca  102  loss  0.5387523  acctest  0.6798418972332015  acctrain  0.7048543689320388\n",
      "epoca  103  loss  0.53712547  acctest  0.6837944664031621  acctrain  0.7067961165048544\n",
      "epoca  104  loss  0.53552854  acctest  0.6996047430830039  acctrain  0.7048543689320388\n",
      "epoca  105  loss  0.53396595  acctest  0.6996047430830039  acctrain  0.7067961165048544\n",
      "epoca  106  loss  0.53241795  acctest  0.6996047430830039  acctrain  0.7126213592233009\n",
      "epoca  107  loss  0.53087735  acctest  0.6956521739130435  acctrain  0.7106796116504854\n",
      "epoca  108  loss  0.5293534  acctest  0.6996047430830039  acctrain  0.7145631067961165\n",
      "epoca  109  loss  0.5278419  acctest  0.7035573122529645  acctrain  0.7203883495145631\n",
      "epoca  110  loss  0.5263458  acctest  0.7035573122529645  acctrain  0.7262135922330097\n",
      "epoca  111  loss  0.52486676  acctest  0.7075098814229249  acctrain  0.7281553398058253\n",
      "epoca  112  loss  0.5234101  acctest  0.7114624505928854  acctrain  0.7281553398058253\n",
      "epoca  113  loss  0.5219619  acctest  0.7154150197628458  acctrain  0.7320388349514563\n",
      "epoca  114  loss  0.52053064  acctest  0.7193675889328063  acctrain  0.7339805825242719\n",
      "epoca  115  loss  0.5191173  acctest  0.7233201581027668  acctrain  0.7398058252427184\n",
      "epoca  116  loss  0.51771116  acctest  0.7233201581027668  acctrain  0.7417475728155339\n",
      "epoca  117  loss  0.516326  acctest  0.7193675889328063  acctrain  0.7456310679611651\n",
      "epoca  118  loss  0.5149726  acctest  0.7233201581027668  acctrain  0.7456310679611651\n",
      "epoca  119  loss  0.5136451  acctest  0.7193675889328063  acctrain  0.7475728155339806\n",
      "epoca  120  loss  0.51234245  acctest  0.7233201581027668  acctrain  0.7495145631067961\n",
      "epoca  121  loss  0.51105475  acctest  0.7312252964426877  acctrain  0.7495145631067961\n",
      "epoca  122  loss  0.509798  acctest  0.7312252964426877  acctrain  0.7553398058252427\n",
      "epoca  123  loss  0.5085686  acctest  0.7312252964426877  acctrain  0.7572815533980582\n",
      "epoca  124  loss  0.5073651  acctest  0.7272727272727273  acctrain  0.7572815533980582\n",
      "epoca  125  loss  0.5061854  acctest  0.7312252964426877  acctrain  0.7572815533980582\n",
      "epoca  126  loss  0.5050334  acctest  0.7312252964426877  acctrain  0.7611650485436893\n",
      "epoca  127  loss  0.50390685  acctest  0.7272727272727273  acctrain  0.7611650485436893\n",
      "epoca  128  loss  0.50279546  acctest  0.7272727272727273  acctrain  0.7631067961165049\n",
      "epoca  129  loss  0.5017015  acctest  0.7312252964426877  acctrain  0.7650485436893204\n",
      "epoca  130  loss  0.5006275  acctest  0.7391304347826086  acctrain  0.7689320388349514\n",
      "epoca  131  loss  0.4995775  acctest  0.7430830039525692  acctrain  0.7689320388349514\n",
      "epoca  132  loss  0.4985582  acctest  0.7470355731225297  acctrain  0.7689320388349514\n",
      "epoca  133  loss  0.49756405  acctest  0.7509881422924901  acctrain  0.7669902912621359\n",
      "epoca  134  loss  0.49658957  acctest  0.7549407114624506  acctrain  0.7669902912621359\n",
      "epoca  135  loss  0.49562573  acctest  0.7509881422924901  acctrain  0.7650485436893204\n",
      "epoca  136  loss  0.49467695  acctest  0.7549407114624506  acctrain  0.7631067961165049\n",
      "epoca  137  loss  0.49374372  acctest  0.7549407114624506  acctrain  0.7631067961165049\n",
      "epoca  138  loss  0.49282953  acctest  0.7549407114624506  acctrain  0.7631067961165049\n",
      "epoca  139  loss  0.49193707  acctest  0.7549407114624506  acctrain  0.7650485436893204\n",
      "epoca  140  loss  0.49106365  acctest  0.758893280632411  acctrain  0.7650485436893204\n",
      "epoca  141  loss  0.49020386  acctest  0.7628458498023716  acctrain  0.7631067961165049\n",
      "epoca  142  loss  0.4893568  acctest  0.766798418972332  acctrain  0.7669902912621359\n",
      "epoca  143  loss  0.48853016  acctest  0.766798418972332  acctrain  0.7669902912621359\n",
      "epoca  144  loss  0.48772556  acctest  0.7628458498023716  acctrain  0.7669902912621359\n",
      "epoca  145  loss  0.4869431  acctest  0.766798418972332  acctrain  0.7708737864077669\n",
      "epoca  146  loss  0.48618183  acctest  0.766798418972332  acctrain  0.7708737864077669\n",
      "epoca  147  loss  0.48543817  acctest  0.766798418972332  acctrain  0.7708737864077669\n",
      "epoca  148  loss  0.48471376  acctest  0.766798418972332  acctrain  0.7747572815533981\n",
      "epoca  149  loss  0.4840115  acctest  0.766798418972332  acctrain  0.7747572815533981\n",
      "epoca  150  loss  0.4833265  acctest  0.766798418972332  acctrain  0.7747572815533981\n",
      "epoca  151  loss  0.48265997  acctest  0.766798418972332  acctrain  0.7708737864077669\n",
      "epoca  152  loss  0.4820071  acctest  0.766798418972332  acctrain  0.7689320388349514\n",
      "epoca  153  loss  0.48137182  acctest  0.7707509881422925  acctrain  0.7708737864077669\n",
      "epoca  154  loss  0.4807552  acctest  0.7707509881422925  acctrain  0.7708737864077669\n",
      "epoca  155  loss  0.48015013  acctest  0.7707509881422925  acctrain  0.7708737864077669\n",
      "epoca  156  loss  0.47955945  acctest  0.7707509881422925  acctrain  0.7708737864077669\n",
      "epoca  157  loss  0.47898346  acctest  0.7707509881422925  acctrain  0.7689320388349514\n",
      "epoca  158  loss  0.47842282  acctest  0.7707509881422925  acctrain  0.7689320388349514\n",
      "epoca  159  loss  0.4778752  acctest  0.7747035573122529  acctrain  0.7728155339805826\n",
      "epoca  160  loss  0.47734398  acctest  0.7747035573122529  acctrain  0.7728155339805826\n",
      "epoca  161  loss  0.47682786  acctest  0.7747035573122529  acctrain  0.7689320388349514\n",
      "epoca  162  loss  0.47632366  acctest  0.7747035573122529  acctrain  0.7689320388349514\n",
      "epoca  163  loss  0.47583294  acctest  0.7747035573122529  acctrain  0.7689320388349514\n",
      "epoca  164  loss  0.47535887  acctest  0.7747035573122529  acctrain  0.7689320388349514\n",
      "epoca  165  loss  0.47489804  acctest  0.7747035573122529  acctrain  0.7669902912621359\n",
      "epoca  166  loss  0.4744472  acctest  0.7786561264822134  acctrain  0.7669902912621359\n",
      "epoca  167  loss  0.47400543  acctest  0.782608695652174  acctrain  0.7689320388349514\n",
      "epoca  168  loss  0.4735689  acctest  0.782608695652174  acctrain  0.7708737864077669\n",
      "epoca  169  loss  0.47314405  acctest  0.782608695652174  acctrain  0.7728155339805826\n",
      "epoca  170  loss  0.47272488  acctest  0.782608695652174  acctrain  0.7728155339805826\n",
      "epoca  171  loss  0.4723093  acctest  0.7786561264822134  acctrain  0.7728155339805826\n",
      "epoca  172  loss  0.47190106  acctest  0.7747035573122529  acctrain  0.7728155339805826\n",
      "epoca  173  loss  0.47150072  acctest  0.7747035573122529  acctrain  0.7728155339805826\n",
      "epoca  174  loss  0.47111169  acctest  0.7747035573122529  acctrain  0.7747572815533981\n",
      "epoca  175  loss  0.47072956  acctest  0.7747035573122529  acctrain  0.7766990291262136\n",
      "epoca  176  loss  0.470359  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "epoca  177  loss  0.46999514  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "epoca  178  loss  0.46963483  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "epoca  179  loss  0.46927962  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "epoca  180  loss  0.46893448  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "epoca  181  loss  0.4685964  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  182  loss  0.46826777  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  183  loss  0.4679518  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  184  loss  0.46764532  acctest  0.782608695652174  acctrain  0.7747572815533981\n",
      "epoca  185  loss  0.46734756  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "epoca  186  loss  0.46705666  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "epoca  187  loss  0.46676153  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  188  loss  0.46647272  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  189  loss  0.4661888  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  190  loss  0.46590924  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  191  loss  0.4656336  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  192  loss  0.46536088  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  193  loss  0.4650917  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  194  loss  0.46482557  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  195  loss  0.46456522  acctest  0.7786561264822134  acctrain  0.7766990291262136\n",
      "epoca  196  loss  0.46431226  acctest  0.7786561264822134  acctrain  0.7766990291262136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoca  197  loss  0.46406308  acctest  0.7786561264822134  acctrain  0.7786407766990291\n",
      "epoca  198  loss  0.46381685  acctest  0.7786561264822134  acctrain  0.7786407766990291\n",
      "epoca  199  loss  0.46357346  acctest  0.7786561264822134  acctrain  0.7747572815533981\n",
      "Acuracia: 0.779\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepara os dados\n",
    "path = 'pima-indians-diabetes.csv'\n",
    "\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "\n",
    "# criar modelo\n",
    "model = MLP(8)\n",
    "# treina o modelo\n",
    "train_model(train_dl, model)\n",
    "# avalia\n",
    "acc = evaluate_model(test_dl, model)\n",
    "print('Acuracia: %.3f' % acc)\n",
    "# testa uma predição\n",
    "#row = [1,0,0.99539,-0.05889,0.85243,0.02306,0.83398,-0.37708,1,0.03760,0.85243,-0.17755,0.59755,-0.44945,0.60536,-0.38223,0.84356,-0.38542,0.58212,-0.32192,0.56971,-0.29674,0.36946,-0.47357,0.56811,-0.51171,0.41078,-0.46168,0.21266,-0.34090,0.42267,-0.54487,0.18641,-0.45300]\n",
    "#yhat = predict(row, model)\n",
    "#print('Predicted: %.3f (class=%d)' % (yhat, yhat.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão com PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        df['originL']= le.fit_transform(df['origin'])\n",
    "\n",
    "        df=df.drop(columns = ['name','origin'])\n",
    "        #df\n",
    "\n",
    "        target_column = ['mpg'] \n",
    "        predictors = list(set(list(df.columns))-set(target_column))\n",
    "        df[predictors] = df[predictors]/df[predictors].max()\n",
    "        #df.describe()\n",
    "\n",
    "        X = df[predictors].values\n",
    "        y = df[target_column].values\n",
    "\n",
    "\n",
    "        X = X.astype('float32')\n",
    "        y = y.astype('float32')\n",
    "\n",
    "\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n",
    "        #print(X_train.shape); print(X_test.shape)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        '''\n",
    "        self.X = df.values[:, :-1]\n",
    "        self.y = df.values[:, -1]\n",
    "        \n",
    "        self.X = self.X.astype('float32')\n",
    "\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "        '''\n",
    " \n",
    "    # quantas linhas tem no dataset?\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # obtem uma linha do dataset\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    # retorna base para treino e teste\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        return random_split(self, [train_size, test_size])\n",
    "    \n",
    "def prepare_data(path):\n",
    "    # Carrega Dataset\n",
    "    dataset = AutoDataset(path)\n",
    "    # realiza split\n",
    "    train, test = dataset.get_splits()\n",
    "    # monta data loaders\n",
    "    train_dl = DataLoader(train, batch_size=1024, shuffle=True)\n",
    "    #train_dl = DataLoader(train)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    #test_dl = DataLoader(test)\n",
    "    #test_dl = DataLoader(test, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train_dl, test_dl\n",
    "\n",
    "class MLP(Module):\n",
    "    # Elementos do modelo\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # camada de entrada\n",
    "        self.hidden1 = Linear(n_inputs, 360)\n",
    "        # Inicialização da camada de entrada\n",
    "        #kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        # Ativação da camada de entrada\n",
    "        self.act1 = ReLU()\n",
    "        # segunda camada , entrada tem que ser do mesmo tamanho da saida da camada 1\n",
    "        self.hidden2 = Linear(360, 128)\n",
    "        # Inicialização da camada\n",
    "        #kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        # Ativação da camada\n",
    "        self.act2 = ReLU()\n",
    "        # camada de saída\n",
    "        self.hidden3 = Linear(128, 1)\n",
    "        # Inicialização da camada\n",
    "        #xavier_uniform_(self.hidden3.weight)\n",
    "        # Ativação da camada\n",
    "        self.act3 = ReLU()\n",
    " \n",
    "    # propagação da entrada pelas camadas\n",
    "    def forward(self, X):\n",
    "        #print(X.shape)\n",
    "        # entrada para primeira camada escondida\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "        # segunda camada escondida\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        # terceira camada escondida\n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        return X\n",
    "\n",
    "logDir = \"/home/silvio/tb/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/train/\"\n",
    "\n",
    "writerTrain = SummaryWriter(logDir)\n",
    "logDir = \"/home/silvio/tb/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/test/\"\n",
    "writerTest = SummaryWriter(logDir)\n",
    "\n",
    "def train_model(train_dl, model):\n",
    "    # define loss\n",
    "    criterion = MSELoss()\n",
    "    # define otimizador\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # loop por épocas\n",
    "    for epoch in range(200):\n",
    "        \n",
    "        # Loop em conjunto de mini-batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            yhat = model(inputs)\n",
    "            # calcula loss\n",
    "            loss = criterion(yhat, targets)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #writer.add_scalar('epoch/mse', loss.detach().numpy(), epoch)\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "            #print('gradiente do peso antes backward', model.hidden1.weight.grad)\n",
    "            #print('pesos antes backward', model.hidden1.weight)\n",
    "            #print('gradiente do bias antes backward', model.hidden1.bias.grad)\n",
    "            #print('bias antes backward', model.hidden1.bias)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            #print('gradiente do peso depois backward', model.hidden1.weight.grad)\n",
    "            #print('pesos depois backward', model.hidden1.weight)\n",
    "            #print('gradiente do bias depois backward', model.hidden1.bias.grad)\n",
    "            #print('bias depois backward', model.hidden1.bias)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            #print('pesos depois passo de otimização', model.hidden1.weight)\n",
    "            #print('bias depois passo de otimização', model.hidden1.bias)\n",
    "            mse=evaluate_model(test_dl, model)\n",
    "            print(\"epoca \" , epoch, \" loss \", loss.detach().numpy(), \"mse test\", mse)\n",
    "            \n",
    "            writerTrain.add_scalar('epoch_mse', loss.detach().numpy(), epoch)\n",
    "            writerTest.add_scalar('epoch_mse', mse, epoch)\n",
    "            \n",
    "def evaluate_model(test_dl, model):\n",
    "    # Cria lista de preditos e reais\n",
    "    predictions, actuals = list(), list()\n",
    "\n",
    "    # percorre lista do dataloader de test\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # realiza predição\n",
    "        yhat = model(inputs)\n",
    "        # cria numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round valores da classe\n",
    "        yhat = yhat.round()\n",
    "        # armazena\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "        \n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # usa sklearn para calcular acurácia\n",
    "    acc = mean_squared_error(actuals, predictions)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266 131\n",
      "epoca  0  loss  609.3169 mse test 437.0005\n",
      "epoca  1  loss  437.4828 mse test 936.2203\n",
      "epoca  2  loss  992.96405 mse test 525.33636\n",
      "epoca  3  loss  545.3559 mse test 522.59436\n",
      "epoca  4  loss  519.27655 mse test 480.5226\n",
      "epoca  5  loss  478.21167 mse test 417.8768\n",
      "epoca  6  loss  421.60864 mse test 333.36533\n",
      "epoca  7  loss  332.68753 mse test 445.5547\n",
      "epoca  8  loss  439.6654 mse test 78.13939\n",
      "epoca  9  loss  77.10262 mse test 9472.208\n",
      "epoca  10  loss  9759.719 mse test 238.7394\n",
      "epoca  11  loss  234.10428 mse test 215.94856\n",
      "epoca  12  loss  229.95277 mse test 215.94856\n",
      "epoca  13  loss  219.01932 mse test 166.2371\n",
      "epoca  14  loss  164.5963 mse test 268.05084\n",
      "epoca  15  loss  295.03973 mse test 191.87679\n",
      "epoca  16  loss  184.44112 mse test 169.80502\n",
      "epoca  17  loss  169.51866 mse test 149.73328\n",
      "epoca  18  loss  153.64029 mse test 131.66153\n",
      "epoca  19  loss  137.14552 mse test 115.58977\n",
      "epoca  20  loss  121.015915 mse test 101.51801\n",
      "epoca  21  loss  106.10974 mse test 89.44626\n",
      "epoca  22  loss  93.060196 mse test 79.37451\n",
      "epoca  23  loss  82.26728 mse test 71.30275\n",
      "epoca  24  loss  73.90562 mse test 65.23099\n",
      "epoca  25  loss  67.913765 mse test 61.159237\n",
      "epoca  26  loss  62.150047 mse test 36.34397\n",
      "epoca  27  loss  39.14476 mse test 139.87831\n",
      "epoca  28  loss  157.80284 mse test 58.814198\n",
      "epoca  29  loss  62.36563 mse test 59.879845\n",
      "epoca  30  loss  62.809307 mse test 59.073742\n",
      "epoca  31  loss  63.192875 mse test 124.943985\n",
      "epoca  32  loss  124.64129 mse test 103.3287\n",
      "epoca  33  loss  102.83483 mse test 64.872215\n",
      "epoca  34  loss  68.52785 mse test 64.872215\n",
      "epoca  35  loss  69.76454 mse test 64.93023\n",
      "epoca  36  loss  70.64908 mse test 65.50122\n",
      "epoca  37  loss  71.13632 mse test 70.80045\n",
      "epoca  38  loss  71.22352 mse test 65.45695\n",
      "epoca  39  loss  70.94326 mse test 64.872215\n",
      "epoca  40  loss  70.353226 mse test 64.872215\n",
      "epoca  41  loss  69.525665 mse test 64.872215\n",
      "epoca  42  loss  68.52559 mse test 64.872215\n",
      "epoca  43  loss  67.35183 mse test 64.872215\n",
      "epoca  44  loss  65.55576 mse test 53.962288\n",
      "epoca  45  loss  59.963898 mse test 43.024883\n",
      "epoca  46  loss  47.070248 mse test 64.89817\n",
      "epoca  47  loss  70.13485 mse test 59.01572\n",
      "epoca  48  loss  62.762203 mse test 59.01572\n",
      "epoca  49  loss  62.31982 mse test 59.053894\n",
      "epoca  50  loss  62.201435 mse test 59.087482\n",
      "epoca  51  loss  62.329243 mse test 59.087482\n",
      "epoca  52  loss  62.624023 mse test 59.087482\n",
      "epoca  53  loss  63.011505 mse test 61.159237\n",
      "epoca  54  loss  63.42614 mse test 61.159237\n",
      "epoca  55  loss  63.81457 mse test 61.159237\n",
      "epoca  56  loss  64.13626 mse test 61.159237\n",
      "epoca  57  loss  64.364815 mse test 61.159237\n",
      "epoca  58  loss  64.47415 mse test 55.502747\n",
      "epoca  59  loss  58.509712 mse test 138.05696\n",
      "epoca  60  loss  140.02441 mse test 143.202\n",
      "epoca  61  loss  144.9059 mse test 150.40504\n",
      "epoca  62  loss  147.2092 mse test 148.59131\n",
      "epoca  63  loss  147.43524 mse test 152.15161\n",
      "epoca  64  loss  146.25456 mse test 153.289\n",
      "epoca  65  loss  144.44171 mse test 146.88443\n",
      "epoca  66  loss  141.98627 mse test 144.91803\n",
      "epoca  67  loss  139.1781 mse test 141.68748\n",
      "epoca  68  loss  136.42241 mse test 133.09207\n",
      "epoca  69  loss  133.95013 mse test 129.76688\n",
      "epoca  70  loss  130.93288 mse test 122.89359\n",
      "epoca  71  loss  128.22087 mse test 119.241684\n",
      "epoca  72  loss  125.655136 mse test 116.90428\n",
      "epoca  73  loss  122.29332 mse test 44470.543\n",
      "epoca  74  loss  42800.97 mse test 60.943966\n",
      "epoca  75  loss  64.41078 mse test 59.087482\n",
      "epoca  76  loss  62.191357 mse test 61.159237\n",
      "epoca  77  loss  64.60927 mse test 65.23099\n",
      "epoca  78  loss  70.178635 mse test 71.30275\n",
      "epoca  79  loss  77.499695 mse test 79.37451\n",
      "epoca  80  loss  85.34402 mse test 89.44626\n",
      "epoca  81  loss  92.711205 mse test 101.37755\n",
      "epoca  82  loss  98.85761 mse test 101.51801\n",
      "epoca  83  loss  103.30085 mse test 101.51801\n",
      "epoca  84  loss  105.80295 mse test 101.51801\n",
      "epoca  85  loss  106.33879 mse test 101.51801\n",
      "epoca  86  loss  105.05688 mse test 101.51801\n",
      "epoca  87  loss  102.230545 mse test 100.43864\n",
      "epoca  88  loss  98.21851 mse test 89.44626\n",
      "epoca  89  loss  93.41202 mse test 89.05237\n",
      "epoca  90  loss  88.1951 mse test 79.37451\n",
      "epoca  91  loss  82.92225 mse test 77.38366\n",
      "epoca  92  loss  77.88114 mse test 70.88595\n",
      "epoca  93  loss  73.174774 mse test 61.391296\n",
      "epoca  94  loss  64.24474 mse test 31.444735\n",
      "epoca  95  loss  35.239975 mse test 36.769928\n",
      "epoca  96  loss  37.727108 mse test 48.870686\n",
      "epoca  97  loss  54.107666 mse test 280.33328\n",
      "epoca  98  loss  306.62048 mse test 59.901226\n",
      "epoca  99  loss  63.338215 mse test 59.087482\n",
      "epoca  100  loss  62.403313 mse test 59.3745\n",
      "epoca  101  loss  62.20742 mse test 59.01572\n",
      "epoca  102  loss  61.978745 mse test 56.502747\n",
      "epoca  103  loss  59.258533 mse test 46.41878\n",
      "epoca  104  loss  49.188824 mse test 42.843204\n",
      "epoca  105  loss  47.576645 mse test 40.72565\n",
      "epoca  106  loss  45.28617 mse test 43.966873\n",
      "epoca  107  loss  47.457825 mse test 46.434048\n",
      "epoca  108  loss  49.579163 mse test 37.238625\n",
      "epoca  109  loss  38.899952 mse test 43.960762\n",
      "epoca  110  loss  44.420155 mse test 36.664585\n",
      "epoca  111  loss  35.15568 mse test 36.565346\n",
      "epoca  112  loss  40.28468 mse test 32.269157\n",
      "epoca  113  loss  32.225006 mse test 41.145496\n",
      "epoca  114  loss  38.76173 mse test 38.42641\n",
      "epoca  115  loss  37.431858 mse test 33.19435\n",
      "epoca  116  loss  34.74048 mse test 31.85084\n",
      "epoca  117  loss  29.945623 mse test 30.516489\n",
      "epoca  118  loss  27.635937 mse test 27.131758\n",
      "epoca  119  loss  28.197184 mse test 28.661526\n",
      "epoca  120  loss  28.573153 mse test 26.33481\n",
      "epoca  121  loss  23.753544 mse test 30.27374\n",
      "epoca  122  loss  26.759283 mse test 25.455421\n",
      "epoca  123  loss  24.69692 mse test 25.69817\n",
      "epoca  124  loss  23.999523 mse test 24.786718\n",
      "epoca  125  loss  21.716625 mse test 23.13023\n",
      "epoca  126  loss  19.691126 mse test 22.131758\n",
      "epoca  127  loss  20.656347 mse test 20.32565\n",
      "epoca  128  loss  18.810362 mse test 20.834047\n",
      "epoca  129  loss  19.311943 mse test 17.574505\n",
      "epoca  130  loss  15.913886 mse test 18.86\n",
      "epoca  131  loss  16.821182 mse test 16.119543\n",
      "epoca  132  loss  14.113634 mse test 15.090535\n",
      "epoca  133  loss  15.855089 mse test 15.56229\n",
      "epoca  134  loss  14.590399 mse test 14.594352\n",
      "epoca  135  loss  13.658563 mse test 14.096642\n",
      "epoca  136  loss  14.798969 mse test 13.223359\n",
      "epoca  137  loss  12.251296 mse test 14.116488\n",
      "epoca  138  loss  12.836113 mse test 10.536336\n",
      "epoca  139  loss  11.540912 mse test 9.750077\n",
      "epoca  140  loss  11.014703 mse test 11.145496\n",
      "epoca  141  loss  11.407816 mse test 9.582137\n",
      "epoca  142  loss  10.153807 mse test 9.801985\n",
      "epoca  143  loss  10.815282 mse test 10.83252\n",
      "epoca  144  loss  10.177645 mse test 90.47374\n",
      "epoca  145  loss  88.65926 mse test 89.342445\n",
      "epoca  146  loss  92.794876 mse test 31.049314\n",
      "epoca  147  loss  30.06731 mse test 180.7226\n",
      "epoca  148  loss  202.57732 mse test 103.07221\n",
      "epoca  149  loss  104.994484 mse test 108.606575\n",
      "epoca  150  loss  108.23316 mse test 104.896645\n",
      "epoca  151  loss  104.84657 mse test 88.45236\n",
      "epoca  152  loss  93.61572 mse test 70.40351\n",
      "epoca  153  loss  73.94216 mse test 23.53481\n",
      "epoca  154  loss  32.52704 mse test 147.49055\n",
      "epoca  155  loss  163.83376 mse test 74.73786\n",
      "epoca  156  loss  82.05515 mse test 89.10428\n",
      "epoca  157  loss  91.489975 mse test 102.42489\n",
      "epoca  158  loss  100.753456 mse test 106.959236\n",
      "epoca  159  loss  107.361565 mse test 99.13786\n",
      "epoca  160  loss  97.79032 mse test 73.77145\n",
      "epoca  161  loss  74.631294 mse test 50.316486\n",
      "epoca  162  loss  57.349754 mse test 43.759235\n",
      "epoca  163  loss  48.215405 mse test 42.424885\n",
      "epoca  164  loss  50.72157 mse test 44.331757\n",
      "epoca  165  loss  58.80043 mse test 29.275269\n",
      "epoca  166  loss  36.604893 mse test 29.759237\n",
      "epoca  167  loss  33.205055 mse test 34.731754\n",
      "epoca  168  loss  36.724106 mse test 30.823359\n",
      "epoca  169  loss  32.77355 mse test 24.817253\n",
      "epoca  170  loss  26.40438 mse test 33.281376\n",
      "epoca  171  loss  34.72015 mse test 27.724123\n",
      "epoca  172  loss  29.333809 mse test 26.34855\n",
      "epoca  173  loss  27.969358 mse test 30.765343\n",
      "epoca  174  loss  32.255623 mse test 23.689007\n",
      "epoca  175  loss  28.049068 mse test 21.560762\n",
      "epoca  176  loss  24.965477 mse test 27.192827\n",
      "epoca  177  loss  28.978876 mse test 20.531757\n",
      "epoca  178  loss  21.99902 mse test 24.583664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoca  179  loss  25.400188 mse test 22.003511\n",
      "epoca  180  loss  21.648607 mse test 19.23252\n",
      "epoca  181  loss  21.338303 mse test 19.169924\n",
      "epoca  182  loss  20.669212 mse test 19.047787\n",
      "epoca  183  loss  19.314524 mse test 20.281372\n",
      "epoca  184  loss  20.045544 mse test 14.533283\n",
      "epoca  185  loss  16.971474 mse test 16.716488\n",
      "epoca  186  loss  18.894043 mse test 15.05084\n",
      "epoca  187  loss  15.826309 mse test 16.760765\n",
      "epoca  188  loss  17.139921 mse test 14.015726\n",
      "epoca  189  loss  14.250582 mse test 15.368399\n",
      "epoca  190  loss  15.865642 mse test 12.017252\n",
      "epoca  191  loss  13.312222 mse test 13.528704\n",
      "epoca  192  loss  14.38936 mse test 11.876794\n",
      "epoca  193  loss  12.75446 mse test 11.988244\n",
      "epoca  194  loss  13.176078 mse test 11.105803\n",
      "epoca  195  loss  11.918203 mse test 12.02794\n",
      "epoca  196  loss  11.867501 mse test 10.26\n",
      "epoca  197  loss  11.263431 mse test 9.22794\n",
      "epoca  198  loss  10.779435 mse test 9.12107\n",
      "epoca  199  loss  10.652522 mse test 9.374504\n",
      "mse: 9.375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepara os dados\n",
    "path = 'Auto2.csv'\n",
    "\n",
    "train_dl, test_dl = prepare_data(path)\n",
    "\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "\n",
    "# criar modelo\n",
    "model = MLP(7)\n",
    "# treina o modelo\n",
    "train_model(train_dl, model)\n",
    "# avalia\n",
    "mse = evaluate_model(test_dl, model)\n",
    "print('mse: %.3f' % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
